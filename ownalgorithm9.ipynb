{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 completed. Loss: 8804.35546875\n",
      "Episode 29 completed. Loss: 8792.205078125\n",
      "Episode 30 completed. Loss: 8750.978515625\n",
      "Episode 30 completed. Loss: 8772.1875\n",
      "Episode 31 completed. Loss: 8843.6337890625\n",
      "Episode 31 completed. Loss: 8687.078125\n",
      "Episode 31 completed. Loss: 8613.693359375\n",
      "Episode 32 completed. Loss: 8647.7919921875\n",
      "Episode 32 completed. Loss: 8703.396484375\n",
      "Episode 33 completed. Loss: 8718.951171875\n",
      "Episode 33 completed. Loss: 8712.625\n",
      "Episode 34 completed. Loss: 8714.34375\n",
      "Episode 34 completed. Loss: 8741.298828125\n",
      "Episode 35 completed. Loss: 8740.2890625\n",
      "Episode 35 completed. Loss: 8667.9658203125\n",
      "Episode 36 completed. Loss: 8702.33984375\n",
      "Episode 36 completed. Loss: 8541.302734375\n",
      "Episode 36 completed. Loss: 8552.7802734375\n",
      "Episode 37 completed. Loss: 8657.1025390625\n",
      "Episode 37 completed. Loss: 8496.9072265625\n",
      "Episode 37 completed. Loss: 8450.4716796875\n",
      "Episode 38 completed. Loss: 8449.732421875\n",
      "Episode 38 completed. Loss: 8464.67578125\n",
      "Episode 39 completed. Loss: 8435.44921875\n",
      "Episode 39 completed. Loss: 8383.0849609375\n",
      "Episode 40 completed. Loss: 8411.8828125\n",
      "Episode 40 completed. Loss: 8433.2578125\n",
      "Episode 41 completed. Loss: 8412.3369140625\n",
      "Episode 41 completed. Loss: 8450.3125\n",
      "Episode 42 completed. Loss: 8470.4052734375\n",
      "Episode 42 completed. Loss: 8326.03125\n",
      "Episode 42 completed. Loss: 8350.6953125\n",
      "Episode 43 completed. Loss: 8359.6953125\n",
      "Episode 43 completed. Loss: 8191.43896484375\n",
      "Episode 43 completed. Loss: 8325.3408203125\n",
      "Episode 44 completed. Loss: 8493.921875\n",
      "Episode 44 completed. Loss: 8447.6142578125\n",
      "Episode 45 completed. Loss: 8403.30078125\n",
      "Episode 45 completed. Loss: 8299.072265625\n",
      "Episode 46 completed. Loss: 8343.6279296875\n",
      "Episode 46 completed. Loss: 8254.1103515625\n",
      "Episode 47 completed. Loss: 8214.5\n",
      "Episode 47 completed. Loss: 8157.25146484375\n",
      "Episode 48 completed. Loss: 8163.64599609375\n",
      "Episode 48 completed. Loss: 8066.57080078125\n",
      "Episode 49 completed. Loss: 8083.041015625\n",
      "Episode 49 completed. Loss: 8052.79052734375\n",
      "Episode 50 completed. Loss: 8070.89892578125\n",
      "Episode 50 completed. Loss: 8181.80810546875\n",
      "Episode 51 completed. Loss: 8211.845703125\n",
      "Episode 51 completed. Loss: 8186.5927734375\n",
      "Episode 52 completed. Loss: 8182.68798828125\n",
      "Episode 52 completed. Loss: 8068.09765625\n",
      "Episode 53 completed. Loss: 8197.7119140625\n",
      "Episode 53 completed. Loss: 8302.935546875\n",
      "Episode 54 completed. Loss: 8315.5732421875\n",
      "Episode 54 completed. Loss: 8209.4814453125\n",
      "Episode 55 completed. Loss: 8237.1728515625\n",
      "Episode 55 completed. Loss: 8229.3818359375\n",
      "Episode 56 completed. Loss: 8194.7724609375\n",
      "Episode 56 completed. Loss: 8196.81640625\n",
      "Episode 57 completed. Loss: 8209.68359375\n",
      "Episode 57 completed. Loss: 8137.23681640625\n",
      "Episode 58 completed. Loss: 8123.95947265625\n",
      "Episode 58 completed. Loss: 8097.994140625\n",
      "Episode 59 completed. Loss: 8087.62841796875\n",
      "Episode 59 completed. Loss: 8129.1484375\n",
      "Episode 60 completed. Loss: 8085.73486328125\n",
      "Episode 60 completed. Loss: 8033.61474609375\n",
      "Episode 61 completed. Loss: 8129.30078125\n",
      "Episode 61 completed. Loss: 8103.6162109375\n",
      "Episode 62 completed. Loss: 8041.6865234375\n",
      "Episode 62 completed. Loss: 8008.3154296875\n",
      "Episode 63 completed. Loss: 7983.072265625\n",
      "Episode 63 completed. Loss: 7932.783203125\n",
      "Episode 64 completed. Loss: 7968.2373046875\n",
      "Episode 64 completed. Loss: 7906.927734375\n",
      "Episode 65 completed. Loss: 7869.3876953125\n",
      "Episode 65 completed. Loss: 7864.958984375\n",
      "Episode 66 completed. Loss: 7798.8818359375\n",
      "Episode 66 completed. Loss: 7911.2451171875\n",
      "Episode 67 completed. Loss: 7964.82373046875\n",
      "Episode 67 completed. Loss: 7864.05859375\n",
      "Episode 68 completed. Loss: 7976.34912109375\n",
      "Episode 68 completed. Loss: 7881.017578125\n",
      "Episode 69 completed. Loss: 7895.716796875\n",
      "Episode 69 completed. Loss: 7839.453125\n",
      "Episode 70 completed. Loss: 7851.974609375\n",
      "Episode 70 completed. Loss: 7874.908203125\n",
      "Episode 71 completed. Loss: 7873.484375\n",
      "Episode 71 completed. Loss: 7866.9541015625\n",
      "Episode 72 completed. Loss: 7920.12646484375\n",
      "Episode 72 completed. Loss: 7752.17529296875\n",
      "Episode 72 completed. Loss: 7723.00146484375\n",
      "Episode 73 completed. Loss: 7822.060546875\n",
      "Episode 73 completed. Loss: 7799.765625\n",
      "Episode 74 completed. Loss: 7808.8056640625\n",
      "Episode 74 completed. Loss: 7900.0234375\n",
      "Episode 75 completed. Loss: 7842.4091796875\n",
      "Episode 75 completed. Loss: 7761.0458984375\n",
      "Episode 76 completed. Loss: 7816.2626953125\n",
      "Episode 76 completed. Loss: 7820.0634765625\n",
      "Episode 77 completed. Loss: 7874.3232421875\n",
      "Episode 77 completed. Loss: 7766.75537109375\n",
      "Episode 78 completed. Loss: 7790.23828125\n",
      "Episode 78 completed. Loss: 7813.43359375\n",
      "Episode 79 completed. Loss: 7789.78515625\n",
      "Episode 79 completed. Loss: 7786.1591796875\n",
      "Episode 80 completed. Loss: 7828.908203125\n",
      "Episode 80 completed. Loss: 7800.27587890625\n",
      "Episode 81 completed. Loss: 7792.79345703125\n",
      "Episode 81 completed. Loss: 7790.9326171875\n",
      "Episode 82 completed. Loss: 7831.318359375\n",
      "Episode 82 completed. Loss: 7791.4765625\n",
      "Episode 83 completed. Loss: 7801.412109375\n",
      "Episode 83 completed. Loss: 7751.353515625\n",
      "Episode 84 completed. Loss: 7807.744140625\n",
      "Episode 84 completed. Loss: 7771.99267578125\n",
      "Episode 85 completed. Loss: 7851.6396484375\n",
      "Episode 85 completed. Loss: 7792.703125\n",
      "Episode 86 completed. Loss: 7873.7421875\n",
      "Episode 86 completed. Loss: 7815.5244140625\n",
      "Episode 87 completed. Loss: 7855.25048828125\n",
      "Episode 87 completed. Loss: 7861.8740234375\n",
      "Episode 88 completed. Loss: 7889.533203125\n",
      "Episode 88 completed. Loss: 7842.650390625\n",
      "Episode 89 completed. Loss: 7889.72509765625\n",
      "Episode 89 completed. Loss: 7850.23974609375\n",
      "Episode 90 completed. Loss: 7888.328125\n",
      "Episode 90 completed. Loss: 7822.0849609375\n",
      "Episode 91 completed. Loss: 7842.912109375\n",
      "Episode 91 completed. Loss: 7838.0908203125\n",
      "Episode 92 completed. Loss: 7819.7470703125\n",
      "Episode 92 completed. Loss: 7846.3662109375\n",
      "Episode 93 completed. Loss: 7863.845703125\n",
      "Episode 93 completed. Loss: 7912.291015625\n",
      "Episode 94 completed. Loss: 7936.5947265625\n",
      "Episode 94 completed. Loss: 7895.0693359375\n",
      "Episode 95 completed. Loss: 7964.47998046875\n",
      "Episode 95 completed. Loss: 7903.1455078125\n",
      "Episode 96 completed. Loss: 7919.5322265625\n",
      "Episode 96 completed. Loss: 7926.96484375\n",
      "Episode 97 completed. Loss: 8029.58544921875\n",
      "Episode 97 completed. Loss: 8012.54443359375\n",
      "Episode 98 completed. Loss: 8077.33203125\n",
      "Episode 98 completed. Loss: 7972.80859375\n",
      "Episode 99 completed. Loss: 8024.2421875\n",
      "Episode 99 completed. Loss: 8015.7158203125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the device (using GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def instance_generator(n):\n",
    "    '''\n",
    "    Function that generates hard Knapsack problem instances.\n",
    "    Input:\n",
    "        -n: desired size of set of items I, defaulted at 50,000 as we use this number in our study\n",
    "    Returns:\n",
    "        -v: array of values for all i items\n",
    "        -w: array of weights of all i items\n",
    "    \n",
    "    ''' \n",
    "    v = np.round(norm.rvs(100, 10, size=n))\n",
    "    w = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        w[i] = round(norm.rvs(v[i], 5))\n",
    "    return v, w\n",
    "\n",
    "v, w = instance_generator(5)\n",
    "# Problem size set-up\n",
    "N = [10]\n",
    "\n",
    "# Capacity constraint function based on the problem size\n",
    "def W(n):\n",
    "    return 0.45*np.sum(w[0:n])\n",
    "\n",
    "capacity = W(N[0])\n",
    "class NeuralNetworkAgent(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, n_heads=4, n_layers=2, seq_len=1):\n",
    "        super(NeuralNetworkAgent, self).__init__()\n",
    "\n",
    "        # Fully connected layers for initial transformation\n",
    "        self.fc1_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder setup\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Fully connected layers after transformer processing\n",
    "        self.fc2_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2_2 = nn.Linear(hidden_dim, 1)  # Output a single Q-value per row\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial transformation\n",
    "        x = self.relu(self.fc1_1(x))\n",
    "        x = self.relu(self.fc1_2(x))\n",
    "\n",
    "        # Transformer expects input of shape (sequence length, batch size, embedding dimension)\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove the sequence length dimension\n",
    "\n",
    "        # Final transformation to output Q-values\n",
    "        x = self.relu(self.fc2_1(x))\n",
    "        qvalues = self.fc2_2(x).squeeze(-1)  # Output one Q-value per row\n",
    "\n",
    "        return qvalues\n",
    "class Knapsack_environment:\n",
    "    def __init__(self, v=v, w=w, capacity=capacity):\n",
    "        self.v = v\n",
    "        self.w = w\n",
    "        self.free = capacity\n",
    "        self.filled = 0\n",
    "        self.selected = np.zeros(len(v))\n",
    "        self.reset(len(v))\n",
    "\n",
    "    def reset(self, N_items):\n",
    "        v_new, w_new = instance_generator(N_items)\n",
    "        W_new = W(N_items)\n",
    "\n",
    "        self.v = v_new\n",
    "        self.w = w_new\n",
    "        self.ratios = self.v / self.w\n",
    "        self.free = np.full(N_items, W_new)\n",
    "        self.canbeselected = np.zeros(N_items)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.vstack([self.v, self.w, self.ratios, self.free, self.canbeselected]).T\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #print(action)\n",
    "        if np.all(self.canbeselected == 1):\n",
    "            # change epi_finished\n",
    "            return self.get_state(), 0,  True\n",
    "        \n",
    "        if self.canbeselected[action] == 1:\n",
    "            \n",
    "            return self.get_state(), -5, False\n",
    "        \n",
    "        self.canbeselected[action] = 1\n",
    "        reward = self.v[action]\n",
    "        self.free = self.free - np.full(len(self.v), self.w[action])\n",
    "\n",
    "        \n",
    "        self.canbeselected[self.w > self.free] = 1\n",
    "        \n",
    "        done = np.all(self.canbeselected == 1)\n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "env = Knapsack_environment()\n",
    "\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10000)\n",
    "\n",
    "episodes = 100\n",
    "batch_size = 64\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 10\n",
    "\n",
    "env = Knapsack_environment()\n",
    "agent = NeuralNetworkAgent().to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "Future_agent = NeuralNetworkAgent().to(device)\n",
    "Future_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "batch_index = 0\n",
    "history_state = []\n",
    "history_next_state = []\n",
    "history_action = []\n",
    "history_reward = []\n",
    "history_done = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "    N_items = 100\n",
    "    \n",
    "    epi_finished = False\n",
    "    total_reward = 0\n",
    "\n",
    "    state_matrix = env.reset(N_items)\n",
    "    state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "    while not epi_finished:\n",
    "        available_items = np.where(state_matrix[:,4] == 0)[0]\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent.forward(state)\n",
    "                action = q_values.argmax().item()\n",
    "        else:\n",
    "            action = random.choice(available_items)\n",
    "        \n",
    "        next_state, reward, epi_finished = env.step(action)\n",
    "        next_state_matrix = next_state\n",
    "        next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "        \n",
    "        history_state.append(state)\n",
    "        history_next_state.append(next_state)\n",
    "        history_action.append(action)\n",
    "        history_reward.append(reward)\n",
    "        history_done.append(float(epi_finished))\n",
    "        \n",
    "        batch_index += 1\n",
    "        state = next_state\n",
    "        state_matrix = next_state_matrix\n",
    "\n",
    "        if batch_index >= batch_size:\n",
    "            q_valuestates = []\n",
    "            q_values_next_states = []\n",
    "\n",
    "            # Manually calculate q_values and next_q_values using your approach\n",
    "            for q_index in range(batch_index - batch_size, batch_index):\n",
    "                q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                next_q_value = Future_agent(history_next_state[q_index]).max()\n",
    "\n",
    "                q_valuestates.append(q_value)\n",
    "                q_values_next_states.append(next_q_value)\n",
    "\n",
    "            q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "            q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "            rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "            # Calculate the target using next_q_values\n",
    "            with torch.no_grad():\n",
    "                targets = rewards + (1.0 - dones) * gamma * q_values_next_states\n",
    "\n",
    "            targets = targets.float()\n",
    "            loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "    \n",
    "    #print(episode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOING FURTHER WITH TRAINING THE SAME AGENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 completed. Loss: 8199.025390625\n",
      "Episode 22 completed. Loss: 8154.6083984375\n",
      "Episode 23 completed. Loss: 8113.1435546875\n",
      "Episode 23 completed. Loss: 8076.8544921875\n",
      "Episode 24 completed. Loss: 8046.501953125\n",
      "Episode 24 completed. Loss: 8006.2373046875\n",
      "Episode 25 completed. Loss: 7968.662109375\n",
      "Episode 25 completed. Loss: 7918.8154296875\n",
      "Episode 26 completed. Loss: 7886.767578125\n",
      "Episode 26 completed. Loss: 7837.5888671875\n",
      "Episode 27 completed. Loss: 7800.60107421875\n",
      "Episode 27 completed. Loss: 7756.1962890625\n",
      "Episode 28 completed. Loss: 7715.90283203125\n",
      "Episode 28 completed. Loss: 7659.564453125\n",
      "Episode 29 completed. Loss: 7611.90087890625\n",
      "Episode 29 completed. Loss: 7546.068359375\n",
      "Episode 30 completed. Loss: 7470.34423828125\n",
      "Episode 30 completed. Loss: 7421.7099609375\n",
      "Episode 31 completed. Loss: 7357.3544921875\n",
      "Episode 31 completed. Loss: 7285.271484375\n",
      "Episode 32 completed. Loss: 7203.861328125\n",
      "Episode 32 completed. Loss: 7139.8349609375\n",
      "Episode 33 completed. Loss: 7065.21484375\n",
      "Episode 33 completed. Loss: 7002.65625\n",
      "Episode 34 completed. Loss: 6930.85009765625\n",
      "Episode 34 completed. Loss: 6856.046875\n",
      "Episode 35 completed. Loss: 6804.68603515625\n",
      "Episode 35 completed. Loss: 6731.234375\n",
      "Episode 36 completed. Loss: 6643.10546875\n",
      "Episode 36 completed. Loss: 6554.9677734375\n",
      "Episode 37 completed. Loss: 6478.7763671875\n",
      "Episode 37 completed. Loss: 6392.93017578125\n",
      "Episode 38 completed. Loss: 6302.853515625\n",
      "Episode 38 completed. Loss: 6211.02734375\n",
      "Episode 39 completed. Loss: 6150.86328125\n",
      "Episode 39 completed. Loss: 6085.03759765625\n",
      "Episode 40 completed. Loss: 5992.9931640625\n",
      "Episode 40 completed. Loss: 5898.4853515625\n",
      "Episode 41 completed. Loss: 5803.04150390625\n",
      "Episode 41 completed. Loss: 5719.4384765625\n",
      "Episode 42 completed. Loss: 5634.51904296875\n",
      "Episode 42 completed. Loss: 5539.4716796875\n",
      "Episode 43 completed. Loss: 5437.1298828125\n",
      "Episode 43 completed. Loss: 5349.486328125\n",
      "Episode 44 completed. Loss: 5263.81494140625\n",
      "Episode 44 completed. Loss: 5154.66162109375\n",
      "Episode 44 completed. Loss: 5048.056640625\n",
      "Episode 45 completed. Loss: 4962.9775390625\n",
      "Episode 45 completed. Loss: 4884.439453125\n",
      "Episode 46 completed. Loss: 4794.2216796875\n",
      "Episode 46 completed. Loss: 4705.791015625\n",
      "Episode 47 completed. Loss: 4611.3017578125\n",
      "Episode 47 completed. Loss: 4495.1279296875\n",
      "Episode 47 completed. Loss: 4417.93359375\n",
      "Episode 48 completed. Loss: 4327.7431640625\n",
      "Episode 48 completed. Loss: 4225.12353515625\n",
      "Episode 49 completed. Loss: 4128.6650390625\n",
      "Episode 49 completed. Loss: 4027.7822265625\n"
     ]
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-3\n",
    "episodes = 50\n",
    "batch_size = 256\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 10\n",
    "\n",
    "for episode in range(episodes):\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "    N_items = 100\n",
    "    \n",
    "    epi_finished = False\n",
    "    total_reward = 0\n",
    "\n",
    "    state_matrix = env.reset(N_items)\n",
    "    state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "    while not epi_finished:\n",
    "        available_items = np.where(state_matrix[:,4] == 0)[0]\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent.forward(state)\n",
    "                action = q_values.argmax().item()\n",
    "        else:\n",
    "            action = random.choice(available_items)\n",
    "        \n",
    "        next_state, reward, epi_finished = env.step(action)\n",
    "        next_state_matrix = next_state\n",
    "        next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "        \n",
    "        history_state.append(state)\n",
    "        history_next_state.append(next_state)\n",
    "        history_action.append(action)\n",
    "        history_reward.append(reward)\n",
    "        history_done.append(float(epi_finished))\n",
    "        \n",
    "        batch_index += 1\n",
    "        state = next_state\n",
    "        state_matrix = next_state_matrix\n",
    "\n",
    "        if batch_index >= batch_size:\n",
    "            q_valuestates = []\n",
    "            q_values_next_states = []\n",
    "\n",
    "            # Manually calculate q_values and next_q_values using your approach\n",
    "            for q_index in range(batch_index - batch_size, batch_index):\n",
    "                q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                next_q_value = Future_agent(history_next_state[q_index]).max()\n",
    "\n",
    "                q_valuestates.append(q_value)\n",
    "                q_values_next_states.append(next_q_value)\n",
    "\n",
    "            q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "            q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "            rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "            # Calculate the target using next_q_values\n",
    "            with torch.no_grad():\n",
    "                targets = rewards + (1.0 - dones) * gamma * q_values_next_states\n",
    "\n",
    "            targets = targets.float()\n",
    "            loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "    \n",
    "    #print(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Greedy Heuristic Function\n",
    "def greedy_knapsack(env):\n",
    "    \"\"\"\n",
    "    A simple greedy heuristic that selects items based on the value-to-weight ratio.\n",
    "    It selects the item with the highest value-to-weight ratio that fits in the remaining capacity.\n",
    "    \n",
    "    Args:\n",
    "        env: The Knapsack_environment object.\n",
    "    \n",
    "    Returns:\n",
    "        total_value: The total value collected by the greedy heuristic.\n",
    "    \"\"\"\n",
    "    state_matrix = env.get_state()\n",
    "    total_value = 0\n",
    "    remaining_capacity = env.free[0]  # Ensure you are working with the current knapsack capacity\n",
    "    \n",
    "    # Compute value-to-weight ratio for each item\n",
    "    value_to_weight_ratios = env.v / env.w\n",
    "    \n",
    "    # Sort items by value-to-weight ratio in descending order\n",
    "    sorted_items = np.argsort(-value_to_weight_ratios)\n",
    "    \n",
    "    for item in sorted_items:\n",
    "        # Check if the item fits in the remaining capacity\n",
    "        if env.w[item] <= remaining_capacity:\n",
    "            total_value += env.v[item]\n",
    "            remaining_capacity -= env.w[item]\n",
    "    \n",
    "    return total_value\n",
    "def dynamic_programming_knapsack(env):\n",
    "    \"\"\"\n",
    "    A dynamic programming solution for the knapsack problem.\n",
    "    \n",
    "    Args:\n",
    "        env: The Knapsack_environment object.\n",
    "    \n",
    "    Returns:\n",
    "        total_value: The total value collected by the dynamic programming approach.\n",
    "    \"\"\"\n",
    "    v, w = env.v, env.w\n",
    "    capacity = int(env.free[0])  # The knapsack capacity\n",
    "\n",
    "    # Initialize DP table\n",
    "    n = len(v)\n",
    "    dp = np.zeros((n + 1, capacity + 1))\n",
    "\n",
    "    # Build DP table\n",
    "    for i in range(1, n + 1):\n",
    "        for c in range(capacity + 1):\n",
    "            if w[i - 1] <= c:\n",
    "                dp[i][c] = max(dp[i - 1][c], dp[i - 1][c - int(w[i - 1])] + v[i - 1])\n",
    "            else:\n",
    "                dp[i][c] = dp[i - 1][c]\n",
    "\n",
    "    # The result is in dp[n][capacity]\n",
    "    total_value = dp[n][capacity]\n",
    "    return total_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward of the agent over 100 runs: 196.04\n",
      "Average reward of the greedy heuristic over 100 runs: 195.28\n",
      "Average reward of the dynamic programming method over 100 runs: 232.03\n",
      "Average execution time of the agent over 100 runs: 0.0064553930 seconds\n",
      "Average execution time of the greedy heuristic over 100 runs: 0.0000359930 seconds\n",
      "Average execution time of the dynamic programming method over 100 runs: 0.0132765350 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Timing utility\n",
    "def get_execution_time(func, env):\n",
    "    start_time = time.perf_counter()  # Start the timer\n",
    "    result = func(env)\n",
    "    end_time = time.perf_counter()  # End the timer\n",
    "    return result, end_time - start_time  # Return both the result and the time taken\n",
    "\n",
    "def compare_agent_vs_greedy_dp_multiple(agent, env, N_items, runs=100):\n",
    "    \"\"\"\n",
    "    Compares the trained agent's performance against the greedy heuristic and dynamic programming method over multiple runs,\n",
    "    including timing the execution of each method.\n",
    "    \n",
    "    Args:\n",
    "        agent: The trained neural network agent.\n",
    "        env: The Knapsack_environment object.\n",
    "        N_items: The number of items in the knapsack problem.\n",
    "        runs: The number of times to run the comparison.\n",
    "    \n",
    "    Returns:\n",
    "        agent_rewards: A list of total rewards collected by the agent over all runs.\n",
    "        greedy_rewards: A list of total rewards collected by the greedy heuristic over all runs.\n",
    "        dp_rewards: A list of total rewards collected by the dynamic programming approach over all runs.\n",
    "        agent_times: A list of execution times for the agent over all runs.\n",
    "        greedy_times: A list of execution times for the greedy heuristic over all runs.\n",
    "        dp_times: A list of execution times for the dynamic programming approach over all runs.\n",
    "    \"\"\"\n",
    "    agent_rewards = []\n",
    "    greedy_rewards = []\n",
    "    dp_rewards = []\n",
    "    agent_times = []\n",
    "    greedy_times = []\n",
    "    dp_times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        # Run and time the agent\n",
    "        env.reset(N_items)\n",
    "        start_time = time.perf_counter()  # Start the timer for the agent\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        agent_reward = 0\n",
    "        epi_finished = False\n",
    "        while not epi_finished:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent(state)\n",
    "                action = q_values.argmax().item()\n",
    "            next_state_matrix, reward, epi_finished = env.step(action)\n",
    "            agent_reward += reward\n",
    "            state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "        \n",
    "        end_time = time.perf_counter()  # End the timer for the agent\n",
    "        agent_time = end_time - start_time\n",
    "\n",
    "        # Reset the environment for the greedy heuristic and dynamic programming\n",
    "        env.reset(N_items)\n",
    "\n",
    "        # Run and time the greedy heuristic\n",
    "        greedy_reward, greedy_time = get_execution_time(greedy_knapsack, env)\n",
    "\n",
    "        # Run and time the dynamic programming solution\n",
    "        dp_reward, dp_time = get_execution_time(dynamic_programming_knapsack, env)\n",
    "\n",
    "        # Collect rewards and times\n",
    "        agent_rewards.append(agent_reward)\n",
    "        greedy_rewards.append(greedy_reward)\n",
    "        dp_rewards.append(dp_reward)\n",
    "        agent_times.append(agent_time)\n",
    "        greedy_times.append(greedy_time)\n",
    "        dp_times.append(dp_time)\n",
    "\n",
    "    return agent_rewards, greedy_rewards, dp_rewards, agent_times, greedy_times, dp_times\n",
    "\n",
    "\n",
    "# Parameters\n",
    "N_items = 100\n",
    "runs = 100  # Set the number of runs for comparison\n",
    "\n",
    "# Compare agent, greedy, and dynamic programming over multiple runs\n",
    "agent_rewards, greedy_rewards, dp_rewards, agent_times, greedy_times, dp_times = compare_agent_vs_greedy_dp_multiple(agent, env, N_items, runs)\n",
    "\n",
    "# Calculate and print average rewards\n",
    "avg_agent_reward = np.mean(agent_rewards)\n",
    "avg_greedy_reward = np.mean(greedy_rewards)\n",
    "avg_dp_reward = np.mean(dp_rewards)\n",
    "\n",
    "# Calculate and print average execution times\n",
    "avg_agent_time = np.mean(agent_times)\n",
    "avg_greedy_time = np.mean(greedy_times)\n",
    "avg_dp_time = np.mean(dp_times)\n",
    "\n",
    "print(f\"Average reward of the agent over {runs} runs: {avg_agent_reward}\")\n",
    "print(f\"Average reward of the greedy heuristic over {runs} runs: {avg_greedy_reward}\")\n",
    "print(f\"Average reward of the dynamic programming method over {runs} runs: {avg_dp_reward}\")\n",
    "print(f\"Average execution time of the agent over {runs} runs: {avg_agent_time:.10f} seconds\")\n",
    "print(f\"Average execution time of the greedy heuristic over {runs} runs: {avg_greedy_time:.10f} seconds\")\n",
    "print(f\"Average execution time of the dynamic programming method over {runs} runs: {avg_dp_time:.10f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
