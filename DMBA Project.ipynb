{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "Gurobi model optimal solution: [453.0, 1173.0, 2337.0, 4679.0, 11673.0, 23493.0, 35247.0, 47151.0, 70656.0]\n",
      "Gurobi model running times: [0.0009970664978027344, 0.001995563507080078, 0.0039882659912109375, 0.012969255447387695, 0.017532825469970703, 0.0348973274230957, 0.0518496036529541, 0.06946325302124023, 0.09635663032531738]\n",
      "Warning: total DP analysis can be lengthy (around 2 minutes)\n",
      "DP case N=10 analysed!\n",
      "DP case N=25 analysed!\n",
      "DP case N=50 analysed!\n",
      "DP case N=100 analysed!\n",
      "DP case N=250 analysed!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: total DP analysis can be lengthy (around 2 minutes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m N:\n\u001b[1;32m--> 120\u001b[0m     DP_sol \u001b[38;5;241m=\u001b[39m \u001b[43mdyn_prog_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     optimal_solution_dp\u001b[38;5;241m.\u001b[39mappend(DP_sol[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    122\u001b[0m     running_time_dp\u001b[38;5;241m.\u001b[39mappend(DP_sol[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[42], line 98\u001b[0m, in \u001b[0;36mdyn_prog_method\u001b[1;34m(n, v, w)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, W_dp\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m j:\n\u001b[1;32m---> 98\u001b[0m         OPT_table[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOPT_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOPT_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         OPT_table[i][j] \u001b[38;5;241m=\u001b[39m OPT_table[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][j]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import gurobipy as gb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set seed for reproducibility of instances\n",
    "np.random.seed(25)\n",
    "\n",
    "print_selections = False\n",
    "\n",
    "def instance_generator(n=50000):\n",
    "    '''\n",
    "    Function that generates hard Knapsack problem instances.\n",
    "    Input:\n",
    "        -n: desired size of set of items I, defaulted at 50,000 as we use this number in our study\n",
    "    Returns:\n",
    "        -v: array of values for all i items\n",
    "        -w: array of weights of all i items\n",
    "    ''' \n",
    "    v = np.round(norm.rvs(100, 10, size=n))\n",
    "    w = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        w[i] = round(norm.rvs(v[i], 5))\n",
    "    return v, w\n",
    "\n",
    "v, w = instance_generator()\n",
    "\n",
    "# Problem size set-up\n",
    "N = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]\n",
    "\n",
    "# Capacity constraint function based on the problem size\n",
    "def W(n):\n",
    "    return round(0.45*np.sum(w[0:n]))\n",
    "\n",
    "# Binary problem\n",
    "def binary_method(n, v=v, w=w):\n",
    "    '''\n",
    "    Function that runs the Gurobi-binary problem.\n",
    "    Input:\n",
    "        -n: Input problem size\n",
    "    Output:\n",
    "        -obj_val: Outcome of the optimizatoin\n",
    "        -running_time: Time to run the algorithm on the problem size\n",
    "    '''\n",
    "    # Selecting relevant i for problem size\n",
    "    W_gb = W(n)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    m = gb.Model(\"Binary model\")\n",
    "    x = m.addVars(n, vtype=gb.GRB.BINARY, name=\"x\")\n",
    "\n",
    "    m.setObjective(gb.quicksum(v[i]*x[i] for i in range(n)), gb.GRB.MAXIMIZE)\n",
    "\n",
    "    m.addConstr(gb.quicksum(w[i]*x[i] for i in range(n)) <= W_gb)\n",
    "\n",
    "    m.update()\n",
    "    m.Params.LogToConsole = 0\n",
    "    m.optimize()\n",
    "    \n",
    "    obj_val = m.objVal\n",
    "    running_time = time.time() - start_time\n",
    "    selected_items = [i for i in range(n) if x[i].X == 1] \n",
    "    return obj_val, running_time, selected_items\n",
    "\n",
    "optimal_solution_binary = [binary_method(n)[0] for n in N]\n",
    "running_time_binary = [binary_method(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"Gurobi model optimal solution: {optimal_solution_binary}\")\n",
    "print(f\"Gurobi model running times: {running_time_binary}\")\n",
    "\n",
    "if print_selections: \n",
    "    print(f\"Gurobi model selected items:{[binary_method(n)[2] for n in N]}\")\n",
    "\n",
    "# Dynamic programming\n",
    "def dyn_prog_method(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_dp = int(W(n))\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Create table for bottom up dynamic programming\n",
    "    OPT_table = [[0 for i in range(W_dp+1)] for i in range(n+1)]\n",
    "    \n",
    "    # Dynamic programming algorithm\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, W_dp+1):\n",
    "            if w[i-1] <= j:\n",
    "                OPT_table[i][j] = max(OPT_table[i-1][j], v[i-1]+ OPT_table[i-1][int(j-w[i-1])])\n",
    "            else:\n",
    "                OPT_table[i][j] = OPT_table[i-1][j]\n",
    "\n",
    "    # Backtrack to find the selected items\n",
    "    selected_items = []\n",
    "    j = W_dp\n",
    "    for i in range(n, 0, -1):\n",
    "        if OPT_table[i][j] != OPT_table[i - 1][j]:  # Item i-1 is included\n",
    "            selected_items.append(i - 1)  # Add the index of the item\n",
    "            j -= int(w[i - 1])  # Reduce the remaining capacity\n",
    "    selected_items.reverse()  # Reverse the list to get the correct order\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return OPT_table[n][W_dp], running_time, selected_items\n",
    "\n",
    "optimal_solution_dp, running_time_dp, selected_items_dp = [], [], []\n",
    "print(\"Warning: total DP analysis can be lengthy (around 2 minutes)\")\n",
    "for n in N:\n",
    "    DP_sol = dyn_prog_method(n)\n",
    "    optimal_solution_dp.append(DP_sol[0])\n",
    "    running_time_dp.append(DP_sol[1])\n",
    "    selected_items_dp.append(DP_sol[2])\n",
    "    print(f\"DP case N={n} analysed!\")\n",
    "\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"DP optimal solution: {optimal_solution_dp}\")\n",
    "print(f\"DP running times: {running_time_dp}\")\n",
    "if print_selections:\n",
    "    print(f\"DP selected items: {selected_items_dp}\")\n",
    "\n",
    "# Greedy Hueristic\n",
    "def greedy_heuristic(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_gy = W(n)\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Calculate ratios\n",
    "    ratios = [(v[i]/w[i], v[i], w[i]) for i in range(n) ]\n",
    "    ratios.sort(reverse=True)\n",
    "\n",
    "    # initialize empty knapsack\n",
    "    weight_in_knapsack = 0\n",
    "    value_in_knapsack = 0\n",
    "\n",
    "    # fill up iteratively over items in ratio order\n",
    "    for _, value, weight in ratios:\n",
    "        if weight_in_knapsack + weight <= W_gy:\n",
    "            value_in_knapsack += value\n",
    "            weight_in_knapsack += weight\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return value_in_knapsack, running_time\n",
    "\n",
    "optimal_solution_gh = [greedy_heuristic(n)[0] for n in N]\n",
    "running_time_gh = [greedy_heuristic(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"Gurobi model optimal solution: {optimal_solution_gh}\")\n",
    "print(f\"Gurobi model running times: {running_time_gh}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Agent\n",
    "\n",
    "#Initial training values for capacity and size of item set\n",
    "capacity, max_items = W(N[0]), 150\n",
    "\n",
    "# Define the device (using GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetworkAgent(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, n_heads=4, n_layers=2, seq_len=1):\n",
    "        super(NeuralNetworkAgent, self).__init__()\n",
    "\n",
    "        # Fully connected layers for initial transformation\n",
    "        self.fc1_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder setup\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Fully connected layers after transformer processing\n",
    "        self.fc2_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2_2 = nn.Linear(hidden_dim, 1)  # Output a single Q-value per row\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial transformation\n",
    "        x = self.relu(self.fc1_1(x))\n",
    "        x = self.relu(self.fc1_2(x))\n",
    "\n",
    "        # Transformer expects input of shape (sequence length, batch size, embedding dimension)\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove the sequence length dimension\n",
    "\n",
    "        # Final transformation to output Q-values\n",
    "        x = self.relu(self.fc2_1(x))\n",
    "        qvalues = self.fc2_2(x).squeeze(-1)  # Output one Q-value per row\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "# Knapsack environment created through class\n",
    "class Knapsack_environment:\n",
    "    def __init__(self, v=v, w=w, capacity=capacity, max_items=max_items):\n",
    "        self.v = np.concatenate((v, np.zeros(max_items-len(v)))) # concatenate allows for later adaptation of amount of training set items\n",
    "        self.w = np.concatenate((w, np.zeros(max_items-len(v))))\n",
    "        self.free = capacity\n",
    "        self.selected = np.zeros(len(v))\n",
    "        self.reset(len(v))\n",
    "\n",
    "    # Re-set knapsack to empty for new problem\n",
    "    def reset(self, N_items, new_input=True):\n",
    "\n",
    "        if new_input:\n",
    "            v_new, w_new = instance_generator(N_items)\n",
    "            W_new = W(N_items)\n",
    "        else:\n",
    "            v_new, w_new, W_new = self.v, self.w, self.free\n",
    "\n",
    "        self.v = np.concatenate((v_new, np.zeros(max_items-len(v))))\n",
    "        self.w = np.concatenate((w_new, np.ones(max_items-len(v))))\n",
    "        self.ratios =  self.v / self.w\n",
    "        self.free =  np.concatenate((np.full(N_items, W_new), np.zeros(max_items-len(v))))\n",
    "        self.preselected =  np.concatenate((np.zeros(N_items), np.ones(max_items-len(v))))\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.vstack([self.v, self.w, self.ratios, self.free, self.preselected]).T\n",
    "\n",
    "    def step(self, action):\n",
    "        if np.all(self.preselected == 1):\n",
    "            return self.get_state(), 0,  True\n",
    "        \n",
    "        if self.preselected[action] == 1: \n",
    "            return self.get_state(), -5, False # wanting to include a pre-selected items is penalized\n",
    "        \n",
    "        self.preselected[action] = 1\n",
    "        reward = self.v[action] # add reward as value of addd item\n",
    "        self.free = self.free - np.full(len(self.v), self.w[action]) # subtract item weight from leftover capacity\n",
    "        self.preselected[self.w > self.free] = 1 # add item to list of pre-selected items\n",
    "        \n",
    "        done = np.all(self.preselected == 1) # termination upon having inspected all items\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "train = False # CHANGE IF YOU WANT TO RE-TRAIN MODEL\n",
    "model_save_path = \"trained_knapsack_agent.pth\"\n",
    "\n",
    "# Training method variables\n",
    "if train:\n",
    "    episodes = 100\n",
    "    batch_size = 64\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10\n",
    "\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "    # Save trained agent\n",
    "    future_agent = NeuralNetworkAgent().to(device)\n",
    "    future_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    batch_index = 0\n",
    "    history_state = []\n",
    "    history_next_state = []\n",
    "    history_action = []\n",
    "    history_reward = []\n",
    "    history_done = []\n",
    "\n",
    "    # training episodes\n",
    "    for episode in range(episodes):\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "        N_items = 100 # Amount of items included in training\n",
    "        \n",
    "        # Initalise termination status and zero reward\n",
    "        ep_finished = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Get a new state\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        while not ep_finished: \n",
    "            # Epsilon greedy policy\n",
    "            if random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.forward(state)\n",
    "                    action = q_values.argmax().item()\n",
    "            else:\n",
    "                available_items = np.where(state_matrix[:,4] == 0)[0] #only consider selectable items\n",
    "                action = random.choice(available_items)\n",
    "            \n",
    "            next_state, reward, ep_finished = env.step(action)\n",
    "            next_state_matrix = next_state\n",
    "            next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "            \n",
    "            # Add step in episode to history\n",
    "            history_state.append(state)\n",
    "            history_next_state.append(next_state)\n",
    "            history_action.append(action)\n",
    "            history_reward.append(reward)\n",
    "            history_done.append(float(ep_finished))\n",
    "            \n",
    "            # Update to inspect next step\n",
    "            batch_index += 1\n",
    "            state = next_state\n",
    "            state_matrix = next_state_matrix\n",
    "\n",
    "            if batch_index >= batch_size:\n",
    "                q_valuestates = []\n",
    "                q_values_next_states = []\n",
    "\n",
    "                # Manually calculate q_values and next_q_values using your approach\n",
    "                for q_index in range(batch_index - batch_size, batch_index):\n",
    "                    q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                    next_q_value = future_agent(history_next_state[q_index]).max() # find optimal q-value\n",
    "                    q_valuestates.append(q_value)\n",
    "                    q_values_next_states.append(next_q_value)\n",
    "\n",
    "                q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "                q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "                rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "                # Calculate the target using next_q_values\n",
    "                with torch.no_grad():\n",
    "                    targets = rewards + (1.0 - dones) * gamma * q_values_next_states # Bellman equation\n",
    "\n",
    "                targets = targets.float()\n",
    "                loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "                # Update neural network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Store agent locally so it won't need to be trained again\n",
    "    torch.save(agent.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "else:\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    agent.load_state_dict(torch.load(model_save_path))\n",
    "    agent.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_by_ratio(weights, values):\n",
    "#     ratio = [(i, v/w) for i, (v,w) in enumerate(zip(values, weights))]\n",
    "#     sorted_items = sorted(ratio, key=lambda x:x[1], reverse=True)\n",
    "#     sorted_indices = [i for i, _ in sorted_items]\n",
    "\n",
    "#     sorted_weights = [weights[i] for i in sorted_indices]\n",
    "#     sorted_values = [values[i] for i in sorted_indices]\n",
    "#     return sorted_weights, sorted_values\n",
    "\n",
    "# class Knapsack:\n",
    "#     '''\n",
    "#     Class with all relevant functions to accurately portray a knapsack problem.\n",
    "#     '''\n",
    "\n",
    "#     #Initialization function\n",
    "#     def __init__(self, weights, values, capacity):\n",
    "#         self.weights = weights\n",
    "#         self.values = values\n",
    "#         self.capacity = capacity\n",
    "#         self.n_items = len(weights)\n",
    "#         self.memory = deque(maxlen=1000)\n",
    "#         self.reset()\n",
    "    \n",
    "#     #Resetting the environment to an empty knapsack\n",
    "#     def reset(self):\n",
    "#         self.current_weight = 0\n",
    "#         self.current_value = 0\n",
    "#         self.done = False\n",
    "#         self.selected_items = []\n",
    "#         self.inspected_items = set()\n",
    "#         return self.current_status()\n",
    "    \n",
    "#     # Function returns the current status of the knapsacks value and weight\n",
    "#     def current_status(self):\n",
    "#         return np.array([self.current_weight, self.current_value])\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         # Terminate when we have processed all items or have otherwise reached a stopping criterium\n",
    "#         if action >= self.n_items or self.done:\n",
    "#             return self.current_status(), 0, self.selected_items, self.done\n",
    "        \n",
    "#         self.inspected_items.add(action)\n",
    "\n",
    "#         # Outrules the option to select already included items multiple times\n",
    "#         if action in self.selected_items:\n",
    "#             reward = 0\n",
    "#         else:\n",
    "#             # In case of adding an item to the knapsack\n",
    "#             if self.current_weight + self.weights[action] <= self.capacity: # if item actually still fits\n",
    "#                 self.current_weight += self.weights[action] # update weight/leftover capacity\n",
    "#                 self.current_value += self.values[action] # update total item value in knapsack\n",
    "#                 self.selected_items.append(action) # add item to list of included items, which we want to return ultimately\n",
    "#                 reward = self.values[action] #reward associated to adding an item for the RL model is equal to the value the item represents in the regular problem\n",
    "\n",
    "#             else:\n",
    "#                 reward = 0\n",
    "#                 self.done = True # If we do not add the item we do not give a reward. No negative rewards as it is not harmful to not include an item\n",
    "\n",
    "#         if self.current_weight >= self.capacity or len(self.inspected_items) >= self.n_items: \n",
    "#             self.done = True # Stopping criterium if capacity is fulfilled -> update self.done\n",
    "\n",
    "#         return self.current_status(), reward, self.selected_items, self.done\n",
    "    \n",
    "# class DQNAgent:\n",
    "#     '''\n",
    "#     Preparing a Q-learning RL agent\n",
    "#     ''' \n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         self.state_size = state_size #defaults to 2 in other pieces of the code (because we include value and weight as input)\n",
    "#         self.action_size = action_size #defaults to n_items \n",
    "#         self.memory = [] # to store previous action/responses\n",
    "#         self.gamma = 0.95 # discount parameter in the Bellman equation\n",
    "#         self.model = self.create_nn()\n",
    "\n",
    "#     def create_nn(self):\n",
    "#         nn = Sequential() # Make use of a very simple and basic NN-structure, COMPLETELY UNTWEAKED right now\n",
    "#         nn.add(Dense(24, input_dim=self.state_size, activation=\"relu\"))\n",
    "#         nn.add(Dropout(0.25))\n",
    "#         nn.add(Dense(24, activation=\"relu\"))\n",
    "#         nn.add(Dropout(0.25))\n",
    "#         nn.add(Dense(self.action_size, activation=\"linear\"))\n",
    "#         nn.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "#         return nn\n",
    "    \n",
    "#     # Store previous experiences\n",
    "#     def remember(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done)) \n",
    "\n",
    "#     # Make a decision on the best action to take\n",
    "#     def act(self, state, epsilon, selected_items):\n",
    "#         if np.random.rand() <= epsilon: # greedy epsilon policy. In 1% of the cases we perform a random action to explore instead of exploit previous knowledge. Necessary to avoid infinite loops.\n",
    "#             available_actions = [i for i in range(self.action_size) if i not in selected_items]\n",
    "#             if available_actions:\n",
    "#                 return random.choice(available_actions) \n",
    "#             else:\n",
    "#                 return 0 \n",
    "#         Q_vals = self.model.predict(state, verbose=0) # create Q-learning table\n",
    "\n",
    "#         for item in selected_items:\n",
    "#             Q_vals[0][item] = -np.inf # Brute-transform Q-values for strategies that include selecting a previously-included item\n",
    "\n",
    "#         return np.argmax(Q_vals[0]) # return best possible decision\n",
    "    \n",
    "#     # Train agent using earlier experiences\n",
    "#     def replay(self, batch_size):\n",
    "#         if len(self.memory) < batch_size: # when we have gathered enough earlier experiences, proceed to next lines\n",
    "#             return \n",
    "        \n",
    "#         # take random sample of previous experience which is used to update the Q-table\n",
    "#         batch = random.sample(self.memory, batch_size)\n",
    "#         for state, action, reward, next_state, done in batch:\n",
    "#             Q = reward\n",
    "#             if not done:\n",
    "#                 Q += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0]) # Bellman equation\n",
    "#             updated_Q = self.model.predict(state, verbose=0) #Use NN to approximate the new Q-table values.\n",
    "#             updated_Q[0][action] = Q\n",
    "#             self.model.fit(state, updated_Q, epochs=1, verbose=0) #\n",
    "\n",
    "# # agent must be trained. Episode count is set to 100 by default.\n",
    "# def train_agent(weights, values, capacity, episodes=1000, batch_size=16):\n",
    "#     knapsack = Knapsack(weights, values, capacity) #initialise knapsack environment\n",
    "#     q_agent = DQNAgent(state_size=2, action_size=len(weights)) #initialise DQN agent.\n",
    "    \n",
    "#     epsilon = 1.0  # Start with a high exploration rate\n",
    "#     epsilon_decay = 0.995 # Over time epsilon will decrease, meaning more exploitation and less exploration through random steps\n",
    "#     epsilon_min = 0.01\n",
    "    \n",
    "#     for ep in range(episodes):\n",
    "#         state = np.reshape(knapsack.reset(), [1, 2])  # Reset the knapsack at the start of each episode\n",
    "#         selected_items = []\n",
    "#         print(f\"starting {ep}\")\n",
    "\n",
    "#         for iteration in range(100):\n",
    "#             # print(f\"starting iter {iteration}\")\n",
    "#             action = q_agent.act(state, epsilon, selected_items)  # Pass epsilon for exploration\n",
    "#             next_state, reward, selected_items, done = knapsack.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, 2])\n",
    "#             q_agent.remember(state, action, reward, next_state, done) \n",
    "#             state = next_state\n",
    "\n",
    "#             if done: # when we arrive at a stopping criterium:\n",
    "#                 # print(f\"Ep {ep}/{episodes}, score: {knapsack.current_value}, included items: {knapsack.selected_items}\")\n",
    "#                 print(f\"Ep {ep}/{episodes}\")\n",
    "#                 break\n",
    "        \n",
    "#         q_agent.replay(batch_size)\n",
    "        \n",
    "#         # Decay epsilon\n",
    "#         if epsilon > epsilon_min:\n",
    "#             epsilon *= epsilon_decay\n",
    "\n",
    "#     return q_agent\n",
    "\n",
    "# trained_agent.model.save(\"dqn_knapsack_agent.h5\")\n",
    "\n",
    "# imported_trained_agent = DQNAgent(state_size=2, action_size=len(weights))\n",
    "# imported_trained_agent.model = load_model(\"dqn_knapsack_agent.h5\")\n",
    "\n",
    "# weights, values = sort_by_ratio(w_train[:4], v_train)[:4]\n",
    "# print(weights, values)\n",
    "# capacity = W(4)\n",
    "# print(capacity)\n",
    "\n",
    "# def knapsackSolver(n_items, weights, values, capacity, epsilon=0.01, agent=trained_agent):\n",
    "#     weights = weights[:n_items] # cut-off at n_items\n",
    "#     values = values[:n_items] # cut-off at n_items\n",
    "#     knapsack = Knapsack(weights, values, capacity)\n",
    "#     state = np.reshape(knapsack.reset(), [1,2])\n",
    "#     total_val = 0\n",
    "#     selected_items = []\n",
    "\n",
    "#     for item in range(100):\n",
    "#         action = agent.act(state, epsilon, selected_items)\n",
    "#         next_state, reward, selected_items, done = knapsack.step(action)\n",
    "#         total_val += reward\n",
    "#         state = np.reshape(next_state, [1,2])\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     return total_val, selected_items\n",
    "\n",
    "# total_value, selected_items = knapsackSolver(4, weights, values, capacity)\n",
    "\n",
    "# print(f\"Total Value: {total_value}\")\n",
    "# print(f\"Selected Items: {selected_items}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
