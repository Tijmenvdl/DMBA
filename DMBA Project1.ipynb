{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "Gurobi model optimal solution: [453.0, 1173.0, 2337.0, 4679.0, 11673.0, 23493.0, 35247.0, 47151.0, 70656.0]\n",
      "Gurobi model running times: [0.0009717941284179688, 0.0020089149475097656, 0.003991603851318359, 0.015952110290527344, 0.2173154354095459, 0.03556561470031738, 0.047983646392822266, 0.07061147689819336, 0.3459641933441162]\n",
      "Warning: total DP analysis can be lengthy (around 2 minutes)\n",
      "DP case N=10 analysed!\n",
      "DP case N=25 analysed!\n",
      "DP case N=50 analysed!\n",
      "DP case N=100 analysed!\n",
      "DP case N=250 analysed!\n",
      "DP case N=500 analysed!\n",
      "DP case N=750 analysed!\n",
      "DP case N=1000 analysed!\n",
      "DP case N=1500 analysed!\n",
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "DP optimal solution: [453.0, 1173.0, 2337.0, 4679.0, 11673.0, 23494.0, 35247.0, 47152.0, 70661.0]\n",
      "DP running times: [0.001750499999616295, 0.011972600001172395, 0.060263200000918005, 0.21901449999859324, 1.3958136000001105, 5.6041562000027625, 13.154577500001324, 22.928197599998384, 51.19433579999895]\n",
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "Gurobi model optimal solution: [387.0, 1170.0, 2288.0, 4678.0, 11670.0, 23433.0, 35244.0, 47152.0, 70600.0]\n",
      "Gurobi model running times: [5.300000339047983e-06, 1.089999932446517e-05, 2.1500000002561137e-05, 4.480000279727392e-05, 0.00011790000280598179, 0.00024539999867556617, 0.0005877000003238209, 0.0005157999985385686, 0.0007834999996703118]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import gurobipy as gb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set seed for reproducibility of instances\n",
    "np.random.seed(25)\n",
    "\n",
    "print_selections = False\n",
    "\n",
    "def instance_generator(n=50000):\n",
    "    '''\n",
    "    Function that generates hard Knapsack problem instances.\n",
    "    Input:\n",
    "        -n: desired size of set of items I, defaulted at 50,000 as we use this number in our study\n",
    "    Returns:\n",
    "        -v: array of values for all i items\n",
    "        -w: array of weights of all i items\n",
    "    ''' \n",
    "    v = np.round(norm.rvs(100, 10, size=n))\n",
    "    w = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        w[i] = round(norm.rvs(v[i], 5))\n",
    "    return v, w\n",
    "\n",
    "v, w = instance_generator()\n",
    "\n",
    "# Problem size set-up\n",
    "N = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]\n",
    "\n",
    "# Capacity constraint function based on the problem size\n",
    "def W(n):\n",
    "    return round(0.45*np.sum(w[0:n]))\n",
    "\n",
    "# Binary problem\n",
    "def binary_method(n, v=v, w=w):\n",
    "    '''\n",
    "    Function that runs the Gurobi-binary problem.\n",
    "    Input:\n",
    "        -n: Input problem size\n",
    "    Output:\n",
    "        -obj_val: Outcome of the optimizatoin\n",
    "        -running_time: Time to run the algorithm on the problem size\n",
    "    '''\n",
    "    # Selecting relevant i for problem size\n",
    "    W_gb = W(n)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    m = gb.Model(\"Binary model\")\n",
    "    x = m.addVars(n, vtype=gb.GRB.BINARY, name=\"x\")\n",
    "\n",
    "    m.setObjective(gb.quicksum(v[i]*x[i] for i in range(n)), gb.GRB.MAXIMIZE)\n",
    "\n",
    "    m.addConstr(gb.quicksum(w[i]*x[i] for i in range(n)) <= W_gb)\n",
    "\n",
    "    m.update()\n",
    "    m.Params.LogToConsole = 0\n",
    "    m.optimize()\n",
    "    \n",
    "    obj_val = m.objVal\n",
    "    running_time = time.time() - start_time\n",
    "    selected_items = [i for i in range(n) if x[i].X == 1] \n",
    "    return obj_val, running_time, selected_items\n",
    "\n",
    "optimal_solution_binary = [binary_method(n)[0] for n in N]\n",
    "running_time_binary = [binary_method(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"Gurobi model optimal solution: {optimal_solution_binary}\")\n",
    "print(f\"Gurobi model running times: {running_time_binary}\")\n",
    "\n",
    "if print_selections: \n",
    "    print(f\"Gurobi model selected items:{[binary_method(n)[2] for n in N]}\")\n",
    "\n",
    "# Dynamic programming\n",
    "def dyn_prog_method(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_dp = int(W(n))\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Create table for bottom up dynamic programming\n",
    "    OPT_table = [[0 for i in range(W_dp+1)] for i in range(n+1)]\n",
    "    \n",
    "    # Dynamic programming algorithm\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, W_dp+1):\n",
    "            if w[i-1] <= j:\n",
    "                OPT_table[i][j] = max(OPT_table[i-1][j], v[i-1]+ OPT_table[i-1][int(j-w[i-1])])\n",
    "            else:\n",
    "                OPT_table[i][j] = OPT_table[i-1][j]\n",
    "\n",
    "    # Backtrack to find the selected items\n",
    "    selected_items = []\n",
    "    j = W_dp\n",
    "    for i in range(n, 0, -1):\n",
    "        if OPT_table[i][j] != OPT_table[i - 1][j]:  # Item i-1 is included\n",
    "            selected_items.append(i - 1)  # Add the index of the item\n",
    "            j -= int(w[i - 1])  # Reduce the remaining capacity\n",
    "    selected_items.reverse()  # Reverse the list to get the correct order\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return OPT_table[n][W_dp], running_time, selected_items\n",
    "\n",
    "optimal_solution_dp, running_time_dp, selected_items_dp = [], [], []\n",
    "print(\"Warning: total DP analysis can be lengthy (around 2 minutes)\")\n",
    "for n in N:\n",
    "    DP_sol = dyn_prog_method(n)\n",
    "    optimal_solution_dp.append(DP_sol[0])\n",
    "    running_time_dp.append(DP_sol[1])\n",
    "    selected_items_dp.append(DP_sol[2])\n",
    "    print(f\"DP case N={n} analysed!\")\n",
    "\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"DP optimal solution: {optimal_solution_dp}\")\n",
    "print(f\"DP running times: {running_time_dp}\")\n",
    "if print_selections:\n",
    "    print(f\"DP selected items: {selected_items_dp}\")\n",
    "\n",
    "# Greedy Hueristic\n",
    "def greedy_heuristic(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_gy = W(n)\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Calculate ratios\n",
    "    ratios = [(v[i]/w[i], v[i], w[i]) for i in range(n) ]\n",
    "    ratios.sort(reverse=True)\n",
    "\n",
    "    # initialize empty knapsack\n",
    "    weight_in_knapsack = 0\n",
    "    value_in_knapsack = 0\n",
    "\n",
    "    # fill up iteratively over items in ratio order\n",
    "    for _, value, weight in ratios:\n",
    "        if weight_in_knapsack + weight <= W_gy:\n",
    "            value_in_knapsack += value\n",
    "            weight_in_knapsack += weight\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return value_in_knapsack, running_time\n",
    "\n",
    "optimal_solution_gh = [greedy_heuristic(n)[0] for n in N]\n",
    "running_time_gh = [greedy_heuristic(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"GH optimal solution: {optimal_solution_gh}\")\n",
    "print(f\"GH running times: {running_time_gh}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed. Loss: 9550.833984375\n",
      "Episode 4 completed. Loss: 9515.30859375\n",
      "Episode 4 completed. Loss: 9453.5390625\n",
      "Episode 4 completed. Loss: 9356.603515625\n",
      "Episode 4 completed. Loss: 9310.3603515625\n",
      "Episode 5 completed. Loss: 9235.6513671875\n",
      "Episode 5 completed. Loss: 9139.126953125\n",
      "Episode 5 completed. Loss: 9094.5947265625\n",
      "Episode 5 completed. Loss: 9042.69921875\n",
      "Episode 5 completed. Loss: 8940.0556640625\n",
      "Episode 5 completed. Loss: 8856.6376953125\n",
      "Episode 5 completed. Loss: 8749.71875\n",
      "Episode 5 completed. Loss: 8548.228515625\n",
      "Episode 5 completed. Loss: 8354.5576171875\n",
      "Episode 5 completed. Loss: 8398.2275390625\n",
      "Episode 5 completed. Loss: 8309.4306640625\n",
      "Episode 5 completed. Loss: 8298.494140625\n",
      "Episode 5 completed. Loss: 8247.9248046875\n",
      "Episode 5 completed. Loss: 8247.9375\n",
      "Episode 5 completed. Loss: 8241.17578125\n",
      "Episode 6 completed. Loss: 8181.1376953125\n",
      "Episode 6 completed. Loss: 8191.525390625\n",
      "Episode 6 completed. Loss: 8120.73046875\n",
      "Episode 6 completed. Loss: 8130.1611328125\n",
      "Episode 6 completed. Loss: 8096.8291015625\n",
      "Episode 6 completed. Loss: 8007.634765625\n",
      "Episode 6 completed. Loss: 7990.349609375\n",
      "Episode 6 completed. Loss: 7857.7548828125\n",
      "Episode 6 completed. Loss: 7808.22998046875\n",
      "Episode 6 completed. Loss: 7655.4033203125\n",
      "Episode 6 completed. Loss: 7500.5830078125\n",
      "Episode 6 completed. Loss: 7504.10693359375\n",
      "Episode 6 completed. Loss: 7323.03173828125\n",
      "Episode 6 completed. Loss: 7320.42578125\n",
      "Episode 6 completed. Loss: 7261.3896484375\n",
      "Episode 6 completed. Loss: 7190.79541015625\n",
      "Episode 7 completed. Loss: 7089.88232421875\n",
      "Episode 7 completed. Loss: 7076.46484375\n",
      "Episode 7 completed. Loss: 7030.4755859375\n",
      "Episode 7 completed. Loss: 6956.33544921875\n",
      "Episode 7 completed. Loss: 6961.3525390625\n",
      "Episode 7 completed. Loss: 6932.34765625\n",
      "Episode 7 completed. Loss: 6864.8203125\n",
      "Episode 7 completed. Loss: 6773.865234375\n",
      "Episode 7 completed. Loss: 6738.2587890625\n",
      "Episode 7 completed. Loss: 6631.70556640625\n",
      "Episode 7 completed. Loss: 6532.294921875\n",
      "Episode 7 completed. Loss: 6455.42724609375\n",
      "Episode 7 completed. Loss: 6416.8525390625\n",
      "Episode 7 completed. Loss: 6234.5888671875\n",
      "Episode 7 completed. Loss: 6163.20361328125\n",
      "Episode 8 completed. Loss: 6075.73828125\n",
      "Episode 8 completed. Loss: 5911.240234375\n",
      "Episode 8 completed. Loss: 5944.625\n",
      "Episode 8 completed. Loss: 5907.55419921875\n",
      "Episode 8 completed. Loss: 5746.60693359375\n",
      "Episode 8 completed. Loss: 5738.8154296875\n",
      "Episode 8 completed. Loss: 5695.416015625\n",
      "Episode 8 completed. Loss: 5621.82568359375\n",
      "Episode 8 completed. Loss: 5586.75\n",
      "Episode 8 completed. Loss: 5504.931640625\n",
      "Episode 8 completed. Loss: 5298.4228515625\n",
      "Episode 8 completed. Loss: 5239.35205078125\n",
      "Episode 8 completed. Loss: 5227.39892578125\n",
      "Episode 8 completed. Loss: 5251.67626953125\n",
      "Episode 8 completed. Loss: 5172.6650390625\n",
      "Episode 9 completed. Loss: 5112.5078125\n",
      "Episode 9 completed. Loss: 5050.77294921875\n",
      "Episode 9 completed. Loss: 4975.7109375\n",
      "Episode 9 completed. Loss: 4953.9677734375\n",
      "Episode 9 completed. Loss: 4885.5390625\n",
      "Episode 9 completed. Loss: 4842.36328125\n",
      "Episode 9 completed. Loss: 4834.46630859375\n",
      "Episode 9 completed. Loss: 4790.85498046875\n",
      "Episode 9 completed. Loss: 4735.3720703125\n",
      "Episode 9 completed. Loss: 4685.30859375\n",
      "Episode 9 completed. Loss: 4662.1572265625\n",
      "Episode 9 completed. Loss: 4671.32421875\n",
      "Episode 9 completed. Loss: 4571.82421875\n",
      "Episode 10 completed. Loss: 4521.32568359375\n",
      "Episode 10 completed. Loss: 4481.8466796875\n",
      "Episode 10 completed. Loss: 4388.849609375\n",
      "Episode 10 completed. Loss: 4310.8623046875\n",
      "Episode 10 completed. Loss: 4218.58544921875\n",
      "Episode 10 completed. Loss: 4164.0986328125\n",
      "Episode 10 completed. Loss: 4065.520263671875\n",
      "Episode 10 completed. Loss: 3963.25341796875\n",
      "Episode 10 completed. Loss: 3836.07666015625\n",
      "Episode 10 completed. Loss: 3752.07080078125\n",
      "Episode 10 completed. Loss: 3610.966552734375\n",
      "Episode 10 completed. Loss: 3559.623291015625\n",
      "Episode 10 completed. Loss: 3507.7646484375\n",
      "Episode 11 completed. Loss: 3485.76611328125\n",
      "Episode 11 completed. Loss: 3453.884765625\n",
      "Episode 11 completed. Loss: 3376.068359375\n",
      "Episode 11 completed. Loss: 3292.218505859375\n",
      "Episode 11 completed. Loss: 3210.81396484375\n",
      "Episode 11 completed. Loss: 3133.35107421875\n",
      "Episode 11 completed. Loss: 3063.21630859375\n",
      "Episode 11 completed. Loss: 2979.94677734375\n",
      "Episode 11 completed. Loss: 2902.9248046875\n",
      "Episode 11 completed. Loss: 2810.843017578125\n",
      "Episode 11 completed. Loss: 2744.6982421875\n",
      "Episode 11 completed. Loss: 2668.959228515625\n",
      "Episode 11 completed. Loss: 2607.04541015625\n",
      "Episode 11 completed. Loss: 2517.638671875\n",
      "Episode 12 completed. Loss: 2494.431396484375\n",
      "Episode 12 completed. Loss: 2427.9931640625\n",
      "Episode 12 completed. Loss: 2341.997314453125\n",
      "Episode 12 completed. Loss: 2290.26025390625\n",
      "Episode 12 completed. Loss: 2220.367431640625\n",
      "Episode 12 completed. Loss: 2159.533203125\n",
      "Episode 12 completed. Loss: 2102.9033203125\n",
      "Episode 12 completed. Loss: 2000.3287353515625\n",
      "Episode 12 completed. Loss: 1938.73583984375\n",
      "Episode 12 completed. Loss: 1911.283935546875\n",
      "Episode 12 completed. Loss: 1806.9913330078125\n",
      "Episode 12 completed. Loss: 1739.7852783203125\n",
      "Episode 12 completed. Loss: 1671.928466796875\n",
      "Episode 13 completed. Loss: 1627.830810546875\n",
      "Episode 13 completed. Loss: 1552.7626953125\n",
      "Episode 13 completed. Loss: 1472.0159912109375\n",
      "Episode 13 completed. Loss: 1423.0654296875\n",
      "Episode 13 completed. Loss: 1362.023193359375\n",
      "Episode 13 completed. Loss: 1286.5272216796875\n",
      "Episode 13 completed. Loss: 1148.6514892578125\n",
      "Episode 13 completed. Loss: 1162.35205078125\n",
      "Episode 13 completed. Loss: 1108.0943603515625\n",
      "Episode 13 completed. Loss: 1065.364501953125\n",
      "Episode 13 completed. Loss: 1030.216796875\n",
      "Episode 13 completed. Loss: 1077.6947021484375\n",
      "Episode 13 completed. Loss: 1136.174072265625\n",
      "Episode 13 completed. Loss: 1100.609130859375\n",
      "Episode 13 completed. Loss: 1161.1273193359375\n",
      "Episode 13 completed. Loss: 1139.8359375\n",
      "Episode 13 completed. Loss: 1107.1971435546875\n",
      "Episode 13 completed. Loss: 1184.22607421875\n",
      "Episode 13 completed. Loss: 1152.591796875\n",
      "Episode 14 completed. Loss: 1146.286376953125\n",
      "Episode 14 completed. Loss: 1139.3934326171875\n",
      "Episode 14 completed. Loss: 1134.7548828125\n",
      "Episode 14 completed. Loss: 1136.97509765625\n",
      "Episode 14 completed. Loss: 1137.814697265625\n",
      "Episode 14 completed. Loss: 1123.8358154296875\n",
      "Episode 14 completed. Loss: 1103.06591796875\n",
      "Episode 14 completed. Loss: 1101.82861328125\n",
      "Episode 14 completed. Loss: 1101.55322265625\n",
      "Episode 14 completed. Loss: 1093.6785888671875\n",
      "Episode 14 completed. Loss: 1211.654296875\n",
      "Episode 14 completed. Loss: 1196.398193359375\n",
      "Episode 14 completed. Loss: 1183.629150390625\n",
      "Episode 14 completed. Loss: 1242.019287109375\n",
      "Episode 14 completed. Loss: 1300.873046875\n",
      "Episode 14 completed. Loss: 1291.4676513671875\n",
      "Episode 14 completed. Loss: 1378.920166015625\n",
      "Episode 15 completed. Loss: 1273.234375\n",
      "Episode 15 completed. Loss: 1293.38232421875\n",
      "Episode 15 completed. Loss: 1263.102783203125\n",
      "Episode 15 completed. Loss: 1240.6121826171875\n",
      "Episode 15 completed. Loss: 1230.179443359375\n",
      "Episode 15 completed. Loss: 1224.7705078125\n",
      "Episode 15 completed. Loss: 1232.861083984375\n",
      "Episode 15 completed. Loss: 1213.028564453125\n",
      "Episode 15 completed. Loss: 1364.64990234375\n",
      "Episode 15 completed. Loss: 1348.201171875\n",
      "Episode 15 completed. Loss: 1335.1031494140625\n",
      "Episode 15 completed. Loss: 1326.218017578125\n",
      "Episode 15 completed. Loss: 1329.765380859375\n",
      "Episode 15 completed. Loss: 1376.8787841796875\n",
      "Episode 15 completed. Loss: 1381.0621337890625\n",
      "Episode 15 completed. Loss: 1466.989013671875\n",
      "Episode 15 completed. Loss: 1427.369140625\n",
      "Episode 16 completed. Loss: 1404.230712890625\n",
      "Episode 16 completed. Loss: 1435.29150390625\n",
      "Episode 16 completed. Loss: 1474.501953125\n",
      "Episode 16 completed. Loss: 1468.0576171875\n",
      "Episode 16 completed. Loss: 1457.8587646484375\n",
      "Episode 16 completed. Loss: 1456.1165771484375\n",
      "Episode 16 completed. Loss: 1413.810546875\n",
      "Episode 16 completed. Loss: 1376.0118408203125\n",
      "Episode 16 completed. Loss: 1315.625732421875\n",
      "Episode 16 completed. Loss: 1347.00927734375\n",
      "Episode 16 completed. Loss: 1331.6890869140625\n",
      "Episode 16 completed. Loss: 1443.709228515625\n",
      "Episode 16 completed. Loss: 1438.45556640625\n",
      "Episode 16 completed. Loss: 1455.650390625\n",
      "Episode 16 completed. Loss: 1529.3966064453125\n",
      "Episode 17 completed. Loss: 1513.093505859375\n",
      "Episode 17 completed. Loss: 1454.852783203125\n",
      "Episode 17 completed. Loss: 1495.998291015625\n",
      "Episode 17 completed. Loss: 1337.208984375\n",
      "Episode 17 completed. Loss: 1394.6571044921875\n",
      "Episode 17 completed. Loss: 1323.040771484375\n",
      "Episode 17 completed. Loss: 1336.360595703125\n",
      "Episode 17 completed. Loss: 1286.119384765625\n",
      "Episode 17 completed. Loss: 1201.016357421875\n",
      "Episode 17 completed. Loss: 1170.311767578125\n",
      "Episode 17 completed. Loss: 1147.72607421875\n",
      "Episode 17 completed. Loss: 1175.8912353515625\n",
      "Episode 17 completed. Loss: 1170.12255859375\n",
      "Episode 17 completed. Loss: 1159.317626953125\n",
      "Episode 18 completed. Loss: 1125.797607421875\n",
      "Episode 18 completed. Loss: 1131.304931640625\n",
      "Episode 18 completed. Loss: 1293.695068359375\n",
      "Episode 18 completed. Loss: 1301.097412109375\n",
      "Episode 18 completed. Loss: 1303.4161376953125\n",
      "Episode 18 completed. Loss: 1464.0440673828125\n",
      "Episode 18 completed. Loss: 1431.8187255859375\n",
      "Episode 18 completed. Loss: 1628.005859375\n",
      "Episode 18 completed. Loss: 1607.101806640625\n",
      "Episode 18 completed. Loss: 1613.619384765625\n",
      "Episode 18 completed. Loss: 1600.75048828125\n",
      "Episode 18 completed. Loss: 1633.644775390625\n",
      "Episode 18 completed. Loss: 1675.41162109375\n",
      "Episode 18 completed. Loss: 1697.1162109375\n",
      "Episode 18 completed. Loss: 1657.637939453125\n",
      "Episode 18 completed. Loss: 1743.992919921875\n",
      "Episode 18 completed. Loss: 1661.125\n",
      "Episode 18 completed. Loss: 1646.736572265625\n",
      "Episode 18 completed. Loss: 1724.4161376953125\n",
      "Episode 19 completed. Loss: 1713.529541015625\n",
      "Episode 19 completed. Loss: 1713.08740234375\n",
      "Episode 19 completed. Loss: 1667.951171875\n",
      "Episode 19 completed. Loss: 1691.679931640625\n",
      "Episode 19 completed. Loss: 1680.35546875\n",
      "Episode 19 completed. Loss: 1659.97705078125\n",
      "Episode 19 completed. Loss: 1690.2310791015625\n",
      "Episode 19 completed. Loss: 1587.5439453125\n",
      "Episode 19 completed. Loss: 1557.0191650390625\n",
      "Episode 19 completed. Loss: 1609.0411376953125\n",
      "Episode 19 completed. Loss: 1568.26171875\n",
      "Episode 19 completed. Loss: 1489.95068359375\n",
      "Episode 19 completed. Loss: 1418.872314453125\n",
      "Model saved to trained_knapsack_agent.pth\n"
     ]
    }
   ],
   "source": [
    "# RL Agent\n",
    "\n",
    "\n",
    "\n",
    "#Initial training values for capacity and size of item set\n",
    "max_items = 150\n",
    "v, w = instance_generator(max_items)\n",
    "capacity = W(max_items)\n",
    "\n",
    "# Define the device (using GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetworkAgent(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, n_heads=4, n_layers=2, seq_len=1):\n",
    "        super(NeuralNetworkAgent, self).__init__()\n",
    "\n",
    "        # Fully connected layers for initial transformation\n",
    "        self.fc1_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder setup\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Fully connected layers after transformer processing\n",
    "        self.fc2_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2_2 = nn.Linear(hidden_dim, 1)  # Output a single Q-value per row\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial transformation\n",
    "        x = self.relu(self.fc1_1(x))\n",
    "        x = self.relu(self.fc1_2(x))\n",
    "\n",
    "        # Transformer expects input of shape (sequence length, batch size, embedding dimension)\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove the sequence length dimension\n",
    "\n",
    "        # Final transformation to output Q-values\n",
    "        x = self.relu(self.fc2_1(x))\n",
    "        qvalues = self.fc2_2(x).squeeze(-1)  # Output one Q-value per row\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "# Knapsack environment created through class\n",
    "class Knapsack_environment:\n",
    "    def __init__(self, v=v, w=w, capacity=capacity, max_items=max_items, new_input=True):\n",
    "        self.v = v # concatenate allows for later adaptation of amount of training set items\n",
    "        self.w = w\n",
    "        self.capacity = capacity\n",
    "        self.free = np.full(len(v), capacity)\n",
    "        print(self.free.shape)\n",
    "        self.selected = np.zeros(len(v))\n",
    "        self.reset(len(v), new_input = new_input)\n",
    "\n",
    "    # Re-set knapsack to empty for new problem\n",
    "    def reset(self, N_items, new_input=True):\n",
    "        \n",
    "        if new_input:\n",
    "            self.v, self.w = instance_generator(N_items)\n",
    "            self.capacity = W(N_items)\n",
    "\n",
    "        '''\n",
    "        self.v = np.concatenate((v_new, np.zeros(max_items-N_items)))\n",
    "        self.w = np.concatenate((w_new, np.ones(max_items-N_items)))\n",
    "        print(w_new.shape)\n",
    "        print(w_new.shape)\n",
    "        self.ratios =  self.v / self.w\n",
    "        \n",
    "        sorted_indices = np.argsort(self.ratios)[::-1]\n",
    "        self.v = self.v[sorted_indices]\n",
    "        self.w = self.w[sorted_indices]\n",
    "        self.ratios = self.ratios[sorted_indices]  # Update ratios after sorting\n",
    "\n",
    "        self.free =  np.concatenate((np.full(N_items, W_new), np.zeros(max_items-N_items)))\n",
    "        self.preselected =  np.concatenate((np.zeros(N_items), np.ones(max_items-N_items)))\n",
    "        '''\n",
    "\n",
    "        self.ratios = self.v / self.w\n",
    "        self.free = np.full(N_items, self.capacity)\n",
    "        self.preselected = np.zeros(N_items)\n",
    "        self.preselected[self.w > self.free] = 1 # add item to list of pre-selected items\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.vstack([self.v, self.w, self.ratios, self.free, self.preselected]).T\n",
    "\n",
    "    def step(self, action):\n",
    "        if np.all(self.preselected == 1):\n",
    "            return self.get_state(), 0,  True\n",
    "        \n",
    "        if self.preselected[action] == 1: \n",
    "            return self.get_state(), -5, False # wanting to include a pre-selected items is penalized\n",
    "        \n",
    "        self.preselected[action] = 1\n",
    "        reward = self.v[action] # add reward as value of addd item\n",
    "        self.free = self.free - np.full(len(self.v), self.w[action]) # subtract item weight from leftover capacity\n",
    "        self.preselected[self.w > self.free] = 1 # add item to list of pre-selected items\n",
    "        \n",
    "        done = np.all(self.preselected == 1) # termination upon having inspected all items\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "train = True # CHANGE IF YOU WANT TO RE-TRAIN MODEL\n",
    "model_save_path = \"trained_knapsack_agent.pth\"\n",
    "\n",
    "# Training method variables\n",
    "if train:\n",
    "    episodes = 20\n",
    "    batch_size = 64\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10\n",
    "\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "    # Save trained agent\n",
    "    future_agent = NeuralNetworkAgent().to(device)\n",
    "    future_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    batch_index = 0\n",
    "    history_state = []\n",
    "    history_next_state = []\n",
    "    history_action = []\n",
    "    history_reward = []\n",
    "    history_done = []\n",
    "\n",
    "    # training episodes\n",
    "    for episode in range(episodes):\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "        N_items = 30 # Amount of items included in training\n",
    "        \n",
    "        # Initalise termination status and zero reward\n",
    "        ep_finished = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Get a new state\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        while not ep_finished: \n",
    "            # Epsilon greedy policy\n",
    "            if random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.forward(state)\n",
    "                    action = q_values.argmax().item()\n",
    "            else:\n",
    "                available_items = np.where(state_matrix[:,4] == 0)[0] #only consider selectable items\n",
    "                action = random.choice(available_items)\n",
    "            \n",
    "            next_state, reward, ep_finished = env.step(action)\n",
    "            next_state_matrix = next_state\n",
    "            next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "            \n",
    "            # Add step in episode to history\n",
    "            history_state.append(state)\n",
    "            history_next_state.append(next_state)\n",
    "            history_action.append(action)\n",
    "            history_reward.append(reward)\n",
    "            history_done.append(float(ep_finished))\n",
    "            \n",
    "            # Update to inspect next step\n",
    "            batch_index += 1\n",
    "            state = next_state\n",
    "            state_matrix = next_state_matrix\n",
    "\n",
    "            if batch_index >= batch_size:\n",
    "                q_valuestates = []\n",
    "                q_values_next_states = []\n",
    "\n",
    "                # Manually calculate q_values and next_q_values using your approach\n",
    "                for q_index in range(batch_index - batch_size, batch_index):\n",
    "                    q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                    next_q_value = future_agent(history_next_state[q_index]).max() # find optimal q-value\n",
    "                    q_valuestates.append(q_value)\n",
    "                    q_values_next_states.append(next_q_value)\n",
    "\n",
    "                q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "                q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "                rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "                # Calculate the target using next_q_values\n",
    "                with torch.no_grad():\n",
    "                    targets = rewards + (1.0 - dones) * gamma * q_values_next_states # Bellman equation\n",
    "\n",
    "                targets = targets.float()\n",
    "                loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "                # Update neural network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Store agent locally so it won't need to be trained again\n",
    "    torch.save(agent.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "else:\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    agent.load_state_dict(torch.load(model_save_path))\n",
    "    agent.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed. Loss: 1519.0625\n",
      "Episode 0 completed. Loss: 1536.22802734375\n",
      "Episode 0 completed. Loss: 1516.185791015625\n",
      "Episode 0 completed. Loss: 1542.08203125\n",
      "Episode 0 completed. Loss: 1553.28759765625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Update neural network parameters\u001b[39;00m\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 77\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(agent\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stijn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-4\n",
    "\n",
    "# Training method variables\n",
    "if train:\n",
    "    episodes = 20\n",
    "    batch_size = 128\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10\n",
    "\n",
    "    # training episodes\n",
    "    for episode in range(episodes):\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "        N_items = 20 # Amount of items included in training\n",
    "        \n",
    "        # Initalise termination status and zero reward\n",
    "        ep_finished = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Get a new state\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        while not ep_finished: \n",
    "            # Epsilon greedy policy\n",
    "            if random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.forward(state)\n",
    "                    action = q_values.argmax().item()\n",
    "            else:\n",
    "                available_items = np.where(state_matrix[:,4] == 0)[0] #only consider selectable items\n",
    "                action = random.choice(available_items)\n",
    "            \n",
    "            next_state, reward, ep_finished = env.step(action)\n",
    "            next_state_matrix = next_state\n",
    "            next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "            \n",
    "            # Add step in episode to history\n",
    "            history_state.append(state)\n",
    "            history_next_state.append(next_state)\n",
    "            history_action.append(action)\n",
    "            history_reward.append(reward)\n",
    "            history_done.append(float(ep_finished))\n",
    "            \n",
    "            # Update to inspect next step\n",
    "            batch_index += 1\n",
    "            state = next_state\n",
    "            state_matrix = next_state_matrix\n",
    "\n",
    "            if batch_index >= batch_size:\n",
    "                q_valuestates = []\n",
    "                q_values_next_states = []\n",
    "\n",
    "                # Manually calculate q_values and next_q_values using your approach\n",
    "                for q_index in range(batch_index - batch_size, batch_index):\n",
    "                    q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                    next_q_value = future_agent(history_next_state[q_index]).max() # find optimal q-value\n",
    "                    q_valuestates.append(q_value)\n",
    "                    q_values_next_states.append(next_q_value)\n",
    "\n",
    "                q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "                q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "                rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "                # Calculate the target using next_q_values\n",
    "                with torch.no_grad():\n",
    "                    targets = rewards + (1.0 - dones) * gamma * q_values_next_states # Bellman equation\n",
    "\n",
    "                targets = targets.float()\n",
    "                loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "                # Update neural network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Store agent locally so it won't need to be trained again\n",
    "    torch.save(agent.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "else:\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    agent.load_state_dict(torch.load(model_save_path))\n",
    "    agent.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "131\n",
      "131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stijn\\AppData\\Local\\Temp\\ipykernel_24120\\3446655868.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load(\"trained_knapsack_agent.pth\")) # Load pre-trained model into it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[131]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knapsackSolver(n, W, v, w, return_all=False):\n",
    "    env = Knapsack_environment(v=v[:n], w=w[:n], capacity=W, new_input=False) # Create appropriate knapsack environment\n",
    "    agent = NeuralNetworkAgent().to(device) # Initialise empty agent\n",
    "    agent.load_state_dict(torch.load(\"trained_knapsack_agent.pth\")) # Load pre-trained model into it\n",
    "    agent.eval() # Set to evaluation mode\n",
    "\n",
    "    state_matrix = env.reset(n, new_input=False)\n",
    "    state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "    selected_items = []\n",
    "    total_value = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.forward(state)\n",
    "            action = q_values.argmax().item() # Choose action with highest Q-value\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # update knapsack entries when item is selected\n",
    "        if reward >= 0:\n",
    "            selected_items.append(action)\n",
    "            total_value += reward\n",
    "        \n",
    "        state_matrix = next_state\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "        \n",
    "        if done:\n",
    "            break # Terminate when all items inspected\n",
    "\n",
    "    running_time = time.perf_counter() - start_time\n",
    "    \n",
    "    if return_all:\n",
    "        return selected_items, total_value, running_time\n",
    "    \n",
    "    return selected_items\n",
    "\n",
    "knapsackSolver(150, W(150), v, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
