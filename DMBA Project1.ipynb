{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "Gurobi model optimal solution: [453.0, 1173.0, 2337.0, 4679.0, 11673.0, 23493.0, 35247.0, 47151.0, 70656.0]\n",
      "Gurobi model running times: [0.0009717941284179688, 0.0020089149475097656, 0.003991603851318359, 0.015952110290527344, 0.2173154354095459, 0.03556561470031738, 0.047983646392822266, 0.07061147689819336, 0.3459641933441162]\n",
      "Warning: total DP analysis can be lengthy (around 2 minutes)\n",
      "DP case N=10 analysed!\n",
      "DP case N=25 analysed!\n",
      "DP case N=50 analysed!\n",
      "DP case N=100 analysed!\n",
      "DP case N=250 analysed!\n",
      "DP case N=500 analysed!\n",
      "DP case N=750 analysed!\n",
      "DP case N=1000 analysed!\n",
      "DP case N=1500 analysed!\n",
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "DP optimal solution: [453.0, 1173.0, 2337.0, 4679.0, 11673.0, 23494.0, 35247.0, 47152.0, 70661.0]\n",
      "DP running times: [0.001750499999616295, 0.011972600001172395, 0.060263200000918005, 0.21901449999859324, 1.3958136000001105, 5.6041562000027625, 13.154577500001324, 22.928197599998384, 51.19433579999895]\n",
      "For n = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]:\n",
      "Gurobi model optimal solution: [387.0, 1170.0, 2288.0, 4678.0, 11670.0, 23433.0, 35244.0, 47152.0, 70600.0]\n",
      "Gurobi model running times: [5.300000339047983e-06, 1.089999932446517e-05, 2.1500000002561137e-05, 4.480000279727392e-05, 0.00011790000280598179, 0.00024539999867556617, 0.0005877000003238209, 0.0005157999985385686, 0.0007834999996703118]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import gurobipy as gb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set seed for reproducibility of instances\n",
    "np.random.seed(25)\n",
    "\n",
    "print_selections = False\n",
    "\n",
    "def instance_generator(n=50000):\n",
    "    '''\n",
    "    Function that generates hard Knapsack problem instances.\n",
    "    Input:\n",
    "        -n: desired size of set of items I, defaulted at 50,000 as we use this number in our study\n",
    "    Returns:\n",
    "        -v: array of values for all i items\n",
    "        -w: array of weights of all i items\n",
    "    ''' \n",
    "    v = np.round(norm.rvs(100, 10, size=n))\n",
    "    w = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        w[i] = round(norm.rvs(v[i], 5))\n",
    "    return v, w\n",
    "\n",
    "v, w = instance_generator()\n",
    "\n",
    "# Problem size set-up\n",
    "N = [10, 25, 50, 100, 250, 500, 750, 1000, 1500]\n",
    "\n",
    "# Capacity constraint function based on the problem size\n",
    "def W(n):\n",
    "    return round(0.45*np.sum(w[0:n]))\n",
    "\n",
    "# Binary problem\n",
    "def binary_method(n, v=v, w=w):\n",
    "    '''\n",
    "    Function that runs the Gurobi-binary problem.\n",
    "    Input:\n",
    "        -n: Input problem size\n",
    "    Output:\n",
    "        -obj_val: Outcome of the optimizatoin\n",
    "        -running_time: Time to run the algorithm on the problem size\n",
    "    '''\n",
    "    # Selecting relevant i for problem size\n",
    "    W_gb = W(n)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    m = gb.Model(\"Binary model\")\n",
    "    x = m.addVars(n, vtype=gb.GRB.BINARY, name=\"x\")\n",
    "\n",
    "    m.setObjective(gb.quicksum(v[i]*x[i] for i in range(n)), gb.GRB.MAXIMIZE)\n",
    "\n",
    "    m.addConstr(gb.quicksum(w[i]*x[i] for i in range(n)) <= W_gb)\n",
    "\n",
    "    m.update()\n",
    "    m.Params.LogToConsole = 0\n",
    "    m.optimize()\n",
    "    \n",
    "    obj_val = m.objVal\n",
    "    running_time = time.time() - start_time\n",
    "    selected_items = [i for i in range(n) if x[i].X == 1] \n",
    "    return obj_val, running_time, selected_items\n",
    "\n",
    "optimal_solution_binary = [binary_method(n)[0] for n in N]\n",
    "running_time_binary = [binary_method(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"Gurobi model optimal solution: {optimal_solution_binary}\")\n",
    "print(f\"Gurobi model running times: {running_time_binary}\")\n",
    "\n",
    "if print_selections: \n",
    "    print(f\"Gurobi model selected items:{[binary_method(n)[2] for n in N]}\")\n",
    "\n",
    "# Dynamic programming\n",
    "def dyn_prog_method(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_dp = int(W(n))\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Create table for bottom up dynamic programming\n",
    "    OPT_table = [[0 for i in range(W_dp+1)] for i in range(n+1)]\n",
    "    \n",
    "    # Dynamic programming algorithm\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, W_dp+1):\n",
    "            if w[i-1] <= j:\n",
    "                OPT_table[i][j] = max(OPT_table[i-1][j], v[i-1]+ OPT_table[i-1][int(j-w[i-1])])\n",
    "            else:\n",
    "                OPT_table[i][j] = OPT_table[i-1][j]\n",
    "\n",
    "    # Backtrack to find the selected items\n",
    "    selected_items = []\n",
    "    j = W_dp\n",
    "    for i in range(n, 0, -1):\n",
    "        if OPT_table[i][j] != OPT_table[i - 1][j]:  # Item i-1 is included\n",
    "            selected_items.append(i - 1)  # Add the index of the item\n",
    "            j -= int(w[i - 1])  # Reduce the remaining capacity\n",
    "    selected_items.reverse()  # Reverse the list to get the correct order\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return OPT_table[n][W_dp], running_time, selected_items\n",
    "\n",
    "optimal_solution_dp, running_time_dp, selected_items_dp = [], [], []\n",
    "print(\"Warning: total DP analysis can be lengthy (around 2 minutes)\")\n",
    "for n in N:\n",
    "    DP_sol = dyn_prog_method(n)\n",
    "    optimal_solution_dp.append(DP_sol[0])\n",
    "    running_time_dp.append(DP_sol[1])\n",
    "    selected_items_dp.append(DP_sol[2])\n",
    "    print(f\"DP case N={n} analysed!\")\n",
    "\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"DP optimal solution: {optimal_solution_dp}\")\n",
    "print(f\"DP running times: {running_time_dp}\")\n",
    "if print_selections:\n",
    "    print(f\"DP selected items: {selected_items_dp}\")\n",
    "\n",
    "# Greedy Hueristic\n",
    "def greedy_heuristic(n, v=v, w=w):\n",
    "    \n",
    "    # Selecting relevant i for problem size, and calculate knapsack capacity\n",
    "    W_gy = W(n)\n",
    "\n",
    "    # Start runtime measurement\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Calculate ratios\n",
    "    ratios = [(v[i]/w[i], v[i], w[i]) for i in range(n) ]\n",
    "    ratios.sort(reverse=True)\n",
    "\n",
    "    # initialize empty knapsack\n",
    "    weight_in_knapsack = 0\n",
    "    value_in_knapsack = 0\n",
    "\n",
    "    # fill up iteratively over items in ratio order\n",
    "    for _, value, weight in ratios:\n",
    "        if weight_in_knapsack + weight <= W_gy:\n",
    "            value_in_knapsack += value\n",
    "            weight_in_knapsack += weight\n",
    "\n",
    "    # End runtime measurement\n",
    "    running_time = time.perf_counter() - start_time\n",
    "\n",
    "    # Return the value in the knapsack and the running time\n",
    "    return value_in_knapsack, running_time\n",
    "\n",
    "optimal_solution_gh = [greedy_heuristic(n)[0] for n in N]\n",
    "running_time_gh = [greedy_heuristic(n)[1] for n in N]\n",
    "print(f\"For n = {N}:\")\n",
    "print(f\"GH optimal solution: {optimal_solution_gh}\")\n",
    "print(f\"GH running times: {running_time_gh}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed. Loss: 9704.2568359375\n",
      "Episode 4 completed. Loss: 9634.3173828125\n",
      "Episode 4 completed. Loss: 9648.9677734375\n",
      "Episode 5 completed. Loss: 9574.583984375\n",
      "Episode 5 completed. Loss: 9521.3515625\n",
      "Episode 5 completed. Loss: 9513.869140625\n",
      "Episode 5 completed. Loss: 9487.00390625\n",
      "Episode 5 completed. Loss: 9340.12890625\n",
      "Episode 5 completed. Loss: 9256.93359375\n",
      "Episode 5 completed. Loss: 9261.7119140625\n",
      "Episode 5 completed. Loss: 9283.478515625\n",
      "Episode 5 completed. Loss: 9220.08984375\n",
      "Episode 5 completed. Loss: 9199.921875\n",
      "Episode 5 completed. Loss: 9180.1396484375\n",
      "Episode 5 completed. Loss: 9164.4755859375\n",
      "Episode 5 completed. Loss: 9125.291015625\n",
      "Episode 5 completed. Loss: 9031.35546875\n",
      "Episode 6 completed. Loss: 9024.296875\n",
      "Episode 6 completed. Loss: 8978.0263671875\n",
      "Episode 6 completed. Loss: 8960.123046875\n",
      "Episode 6 completed. Loss: 9046.39453125\n",
      "Episode 6 completed. Loss: 8864.3984375\n",
      "Episode 6 completed. Loss: 8752.9326171875\n",
      "Episode 6 completed. Loss: 8773.36328125\n",
      "Episode 6 completed. Loss: 8705.6484375\n",
      "Episode 6 completed. Loss: 8664.1416015625\n",
      "Episode 6 completed. Loss: 8564.6640625\n",
      "Episode 6 completed. Loss: 8487.2529296875\n",
      "Episode 6 completed. Loss: 8430.87890625\n",
      "Episode 6 completed. Loss: 8254.15234375\n",
      "Episode 6 completed. Loss: 8236.1328125\n",
      "Episode 6 completed. Loss: 8239.642578125\n",
      "Episode 7 completed. Loss: 8125.33203125\n",
      "Episode 7 completed. Loss: 8064.1923828125\n",
      "Episode 7 completed. Loss: 8001.97705078125\n",
      "Episode 7 completed. Loss: 7933.02978515625\n",
      "Episode 7 completed. Loss: 7919.5634765625\n",
      "Episode 7 completed. Loss: 7864.046875\n",
      "Episode 7 completed. Loss: 7788.1513671875\n",
      "Episode 7 completed. Loss: 7706.73486328125\n",
      "Episode 7 completed. Loss: 7624.1953125\n",
      "Episode 7 completed. Loss: 7578.37939453125\n",
      "Episode 7 completed. Loss: 7523.48388671875\n",
      "Episode 7 completed. Loss: 7494.76904296875\n",
      "Episode 8 completed. Loss: 7445.2822265625\n",
      "Episode 8 completed. Loss: 7379.2431640625\n",
      "Episode 8 completed. Loss: 7334.03515625\n",
      "Episode 8 completed. Loss: 7214.0205078125\n",
      "Episode 8 completed. Loss: 7164.31201171875\n",
      "Episode 8 completed. Loss: 7192.5546875\n",
      "Episode 8 completed. Loss: 7120.61572265625\n",
      "Episode 8 completed. Loss: 7071.2958984375\n",
      "Episode 8 completed. Loss: 6867.208984375\n",
      "Episode 8 completed. Loss: 6787.3994140625\n",
      "Episode 8 completed. Loss: 6713.85546875\n",
      "Episode 8 completed. Loss: 6599.4970703125\n",
      "Episode 8 completed. Loss: 6489.9248046875\n",
      "Episode 8 completed. Loss: 6331.10791015625\n",
      "Episode 8 completed. Loss: 6183.787109375\n",
      "Episode 8 completed. Loss: 6088.8388671875\n",
      "Episode 9 completed. Loss: 6067.23583984375\n",
      "Episode 9 completed. Loss: 6017.7978515625\n",
      "Episode 9 completed. Loss: 5923.81298828125\n",
      "Episode 9 completed. Loss: 5808.953125\n",
      "Episode 9 completed. Loss: 5755.365234375\n",
      "Episode 9 completed. Loss: 5680.1796875\n",
      "Episode 9 completed. Loss: 5511.40185546875\n",
      "Episode 9 completed. Loss: 5484.876953125\n",
      "Episode 9 completed. Loss: 5450.7900390625\n",
      "Episode 9 completed. Loss: 5286.744140625\n",
      "Episode 9 completed. Loss: 5171.82275390625\n",
      "Episode 9 completed. Loss: 5178.826171875\n",
      "Episode 9 completed. Loss: 5040.91162109375\n",
      "Episode 9 completed. Loss: 4887.99365234375\n",
      "Episode 9 completed. Loss: 4794.43359375\n",
      "Episode 9 completed. Loss: 4656.94921875\n",
      "Episode 9 completed. Loss: 4503.21533203125\n",
      "Episode 9 completed. Loss: 4437.28515625\n",
      "Episode 9 completed. Loss: 4389.53466796875\n",
      "Episode 10 completed. Loss: 4313.146484375\n",
      "Episode 10 completed. Loss: 4264.2255859375\n",
      "Episode 10 completed. Loss: 4170.6533203125\n",
      "Episode 10 completed. Loss: 4142.5556640625\n",
      "Episode 10 completed. Loss: 4006.263671875\n",
      "Episode 10 completed. Loss: 3955.873291015625\n",
      "Episode 10 completed. Loss: 3938.99169921875\n",
      "Episode 10 completed. Loss: 3919.114501953125\n",
      "Episode 10 completed. Loss: 3789.973388671875\n",
      "Episode 10 completed. Loss: 3733.70166015625\n",
      "Episode 10 completed. Loss: 3666.058837890625\n",
      "Episode 10 completed. Loss: 3612.049560546875\n",
      "Episode 10 completed. Loss: 3537.264892578125\n",
      "Episode 10 completed. Loss: 3490.139892578125\n",
      "Episode 10 completed. Loss: 3439.4462890625\n",
      "Episode 10 completed. Loss: 3354.9453125\n",
      "Episode 10 completed. Loss: 3312.213623046875\n",
      "Episode 11 completed. Loss: 3274.645751953125\n",
      "Episode 11 completed. Loss: 3185.48486328125\n",
      "Episode 11 completed. Loss: 3115.239990234375\n",
      "Episode 11 completed. Loss: 3091.88232421875\n",
      "Episode 11 completed. Loss: 3002.826171875\n",
      "Episode 11 completed. Loss: 2914.728271484375\n",
      "Episode 11 completed. Loss: 2823.45751953125\n",
      "Episode 11 completed. Loss: 2763.04150390625\n",
      "Episode 11 completed. Loss: 2694.58837890625\n",
      "Episode 11 completed. Loss: 2630.088134765625\n",
      "Episode 11 completed. Loss: 2557.086669921875\n",
      "Episode 11 completed. Loss: 2522.4580078125\n",
      "Episode 11 completed. Loss: 2469.68212890625\n",
      "Episode 11 completed. Loss: 2415.2177734375\n",
      "Episode 11 completed. Loss: 2409.12841796875\n",
      "Episode 11 completed. Loss: 2372.25390625\n",
      "Episode 12 completed. Loss: 2328.62060546875\n",
      "Episode 12 completed. Loss: 2265.753662109375\n",
      "Episode 12 completed. Loss: 2264.117431640625\n",
      "Episode 12 completed. Loss: 2242.0380859375\n",
      "Episode 12 completed. Loss: 2171.898681640625\n",
      "Episode 12 completed. Loss: 2154.79248046875\n",
      "Episode 12 completed. Loss: 2146.64111328125\n",
      "Episode 12 completed. Loss: 2128.537841796875\n",
      "Episode 12 completed. Loss: 2104.56103515625\n",
      "Episode 12 completed. Loss: 2027.7481689453125\n",
      "Episode 12 completed. Loss: 1941.026611328125\n",
      "Episode 12 completed. Loss: 1973.5361328125\n",
      "Episode 12 completed. Loss: 1928.79443359375\n",
      "Episode 13 completed. Loss: 1931.3367919921875\n",
      "Episode 13 completed. Loss: 1920.267333984375\n",
      "Episode 13 completed. Loss: 1919.808837890625\n",
      "Episode 13 completed. Loss: 1898.5045166015625\n",
      "Episode 13 completed. Loss: 1889.40771484375\n",
      "Episode 13 completed. Loss: 1788.256103515625\n",
      "Episode 13 completed. Loss: 1772.258056640625\n",
      "Episode 13 completed. Loss: 1761.6123046875\n",
      "Episode 13 completed. Loss: 1645.105712890625\n",
      "Episode 13 completed. Loss: 1646.421142578125\n",
      "Episode 13 completed. Loss: 1745.9136962890625\n",
      "Episode 13 completed. Loss: 1627.309326171875\n",
      "Episode 13 completed. Loss: 1611.1339111328125\n",
      "Episode 13 completed. Loss: 1606.6435546875\n",
      "Episode 13 completed. Loss: 1565.669189453125\n",
      "Episode 13 completed. Loss: 1434.5087890625\n",
      "Episode 14 completed. Loss: 1432.110107421875\n",
      "Episode 14 completed. Loss: 1354.24609375\n",
      "Episode 14 completed. Loss: 1371.09326171875\n",
      "Episode 14 completed. Loss: 1362.043212890625\n",
      "Episode 14 completed. Loss: 1342.595703125\n",
      "Episode 14 completed. Loss: 1318.7901611328125\n",
      "Episode 14 completed. Loss: 1177.27978515625\n",
      "Episode 14 completed. Loss: 1171.014404296875\n",
      "Episode 14 completed. Loss: 1160.160400390625\n",
      "Episode 14 completed. Loss: 1164.3157958984375\n",
      "Episode 14 completed. Loss: 994.548583984375\n",
      "Episode 14 completed. Loss: 998.1072998046875\n",
      "Episode 14 completed. Loss: 1071.271728515625\n",
      "Episode 14 completed. Loss: 979.492431640625\n",
      "Episode 14 completed. Loss: 909.0657348632812\n",
      "Episode 14 completed. Loss: 937.1422119140625\n",
      "Episode 15 completed. Loss: 915.590576171875\n",
      "Episode 15 completed. Loss: 877.3026123046875\n",
      "Episode 15 completed. Loss: 831.8095703125\n",
      "Episode 15 completed. Loss: 798.766357421875\n",
      "Episode 15 completed. Loss: 809.3941650390625\n",
      "Episode 15 completed. Loss: 776.5951538085938\n",
      "Episode 15 completed. Loss: 814.6895751953125\n",
      "Episode 15 completed. Loss: 956.9683837890625\n",
      "Episode 15 completed. Loss: 963.551025390625\n",
      "Episode 15 completed. Loss: 967.21533203125\n",
      "Episode 15 completed. Loss: 1061.85009765625\n",
      "Episode 15 completed. Loss: 1179.689208984375\n",
      "Episode 15 completed. Loss: 1224.88720703125\n",
      "Episode 15 completed. Loss: 1339.5101318359375\n",
      "Episode 15 completed. Loss: 1245.783203125\n",
      "Episode 15 completed. Loss: 1255.7275390625\n",
      "Episode 15 completed. Loss: 1265.06689453125\n",
      "Episode 15 completed. Loss: 1237.7557373046875\n",
      "Episode 16 completed. Loss: 1187.96484375\n",
      "Episode 16 completed. Loss: 1216.046142578125\n",
      "Episode 16 completed. Loss: 1186.076416015625\n",
      "Episode 16 completed. Loss: 1152.5859375\n",
      "Episode 16 completed. Loss: 1148.947021484375\n",
      "Episode 16 completed. Loss: 1155.691650390625\n",
      "Episode 16 completed. Loss: 1151.04736328125\n",
      "Episode 16 completed. Loss: 1166.8428955078125\n",
      "Episode 16 completed. Loss: 1189.2506103515625\n",
      "Episode 16 completed. Loss: 1184.1337890625\n",
      "Episode 16 completed. Loss: 1158.9317626953125\n",
      "Episode 16 completed. Loss: 1093.56884765625\n",
      "Episode 16 completed. Loss: 1073.388916015625\n",
      "Episode 16 completed. Loss: 1106.173828125\n",
      "Episode 16 completed. Loss: 1177.064697265625\n",
      "Episode 17 completed. Loss: 1116.728515625\n",
      "Episode 17 completed. Loss: 1122.78515625\n",
      "Episode 17 completed. Loss: 1157.623046875\n",
      "Episode 17 completed. Loss: 1138.247802734375\n",
      "Episode 17 completed. Loss: 1121.018310546875\n",
      "Episode 17 completed. Loss: 1126.4912109375\n",
      "Episode 17 completed. Loss: 1268.8389892578125\n",
      "Episode 17 completed. Loss: 1227.012939453125\n",
      "Episode 17 completed. Loss: 1159.189208984375\n",
      "Episode 17 completed. Loss: 1199.593994140625\n",
      "Episode 17 completed. Loss: 1289.008544921875\n",
      "Episode 17 completed. Loss: 1227.2181396484375\n",
      "Episode 17 completed. Loss: 1225.94580078125\n",
      "Episode 17 completed. Loss: 1216.553955078125\n",
      "Episode 17 completed. Loss: 1286.27099609375\n",
      "Episode 17 completed. Loss: 1344.8040771484375\n",
      "Episode 18 completed. Loss: 1298.114501953125\n",
      "Episode 18 completed. Loss: 1320.428466796875\n",
      "Episode 18 completed. Loss: 1303.67578125\n",
      "Episode 18 completed. Loss: 1269.60302734375\n",
      "Episode 18 completed. Loss: 1284.528076171875\n",
      "Episode 18 completed. Loss: 1282.5927734375\n",
      "Episode 18 completed. Loss: 1296.216796875\n",
      "Episode 18 completed. Loss: 1288.6181640625\n",
      "Episode 18 completed. Loss: 1315.27783203125\n",
      "Episode 18 completed. Loss: 1391.248779296875\n",
      "Episode 18 completed. Loss: 1367.7890625\n",
      "Episode 18 completed. Loss: 1409.19140625\n",
      "Episode 18 completed. Loss: 1366.78369140625\n",
      "Episode 18 completed. Loss: 1355.943115234375\n",
      "Episode 18 completed. Loss: 1388.305908203125\n",
      "Episode 18 completed. Loss: 1492.0516357421875\n",
      "Episode 18 completed. Loss: 1487.32568359375\n",
      "Episode 19 completed. Loss: 1497.960693359375\n",
      "Episode 19 completed. Loss: 1470.744140625\n",
      "Episode 19 completed. Loss: 1463.7783203125\n",
      "Episode 19 completed. Loss: 1439.461181640625\n",
      "Episode 19 completed. Loss: 1477.08251953125\n",
      "Episode 19 completed. Loss: 1394.3447265625\n",
      "Episode 19 completed. Loss: 1383.465576171875\n",
      "Episode 19 completed. Loss: 1363.41357421875\n",
      "Episode 19 completed. Loss: 1450.5108642578125\n",
      "Episode 19 completed. Loss: 1294.421875\n",
      "Episode 19 completed. Loss: 1329.3824462890625\n",
      "Episode 19 completed. Loss: 1382.208984375\n",
      "Episode 19 completed. Loss: 1382.506103515625\n",
      "Episode 19 completed. Loss: 1333.25390625\n",
      "Episode 19 completed. Loss: 1433.819091796875\n",
      "Episode 19 completed. Loss: 1418.7921142578125\n",
      "Episode 19 completed. Loss: 1497.1796875\n",
      "Episode 19 completed. Loss: 1501.2120361328125\n",
      "Model saved to trained_knapsack_agent.pth\n"
     ]
    }
   ],
   "source": [
    "# RL Agent\n",
    "\n",
    "\n",
    "\n",
    "#Initial training values for capacity and size of item set\n",
    "max_items = 150\n",
    "v, w = instance_generator(max_items)\n",
    "capacity = W(max_items)\n",
    "\n",
    "# Define the device (using GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetworkAgent(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, n_heads=4, n_layers=2, seq_len=1):\n",
    "        super(NeuralNetworkAgent, self).__init__()\n",
    "\n",
    "        # Fully connected layers for initial transformation\n",
    "        self.fc1_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder setup\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Fully connected layers after transformer processing\n",
    "        self.fc2_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2_2 = nn.Linear(hidden_dim, 1)  # Output a single Q-value per row\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial transformation\n",
    "        x = self.relu(self.fc1_1(x))\n",
    "        x = self.relu(self.fc1_2(x))\n",
    "\n",
    "        # Transformer expects input of shape (sequence length, batch size, embedding dimension)\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove the sequence length dimension\n",
    "\n",
    "        # Final transformation to output Q-values\n",
    "        x = self.relu(self.fc2_1(x))\n",
    "        qvalues = self.fc2_2(x).squeeze(-1)  # Output one Q-value per row\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "# Knapsack environment created through class\n",
    "class Knapsack_environment:\n",
    "    def __init__(self, v=v, w=w, capacity=capacity, max_items=max_items):\n",
    "        self.v = np.concatenate((v, np.zeros(max_items-len(v)))) # concatenate allows for later adaptation of amount of training set items\n",
    "        self.w = np.concatenate((w, np.zeros(max_items-len(v))))\n",
    "        self.free = capacity\n",
    "        self.selected = np.zeros(len(v))\n",
    "        self.reset(len(v))\n",
    "\n",
    "    # Re-set knapsack to empty for new problem\n",
    "    def reset(self, N_items, new_input=True):\n",
    "\n",
    "        if new_input:\n",
    "            v_new, w_new = instance_generator(N_items)\n",
    "            W_new = W(N_items)\n",
    "        else:\n",
    "            v_new, w_new, W_new = self.v, self.w, self.free\n",
    "        \n",
    "\n",
    "        self.v = np.concatenate((v_new, np.zeros(max_items-len(v))))\n",
    "        self.w = np.concatenate((w_new, np.ones(max_items-len(v))))\n",
    "        self.ratios =  self.v / self.w\n",
    "        \n",
    "        sorted_indices = np.argsort(self.ratios)[::-1]\n",
    "        self.v = self.v[sorted_indices]\n",
    "        self.w = self.w[sorted_indices]\n",
    "        self.ratios = self.ratios[sorted_indices]  # Update ratios after sorting\n",
    "\n",
    "\n",
    "        self.free =  np.concatenate((np.full(N_items, W_new), np.zeros(max_items-len(v))))\n",
    "        self.preselected =  np.concatenate((np.zeros(N_items), np.ones(max_items-len(v))))\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.vstack([self.v, self.w, self.ratios, self.free, self.preselected]).T\n",
    "\n",
    "    def step(self, action):\n",
    "        if np.all(self.preselected == 1):\n",
    "            return self.get_state(), 0,  True\n",
    "        \n",
    "        if self.preselected[action] == 1: \n",
    "            return self.get_state(), -5, False # wanting to include a pre-selected items is penalized\n",
    "        \n",
    "        self.preselected[action] = 1\n",
    "        reward = self.v[action] # add reward as value of addd item\n",
    "        self.free = self.free - np.full(len(self.v), self.w[action]) # subtract item weight from leftover capacity\n",
    "        self.preselected[self.w > self.free] = 1 # add item to list of pre-selected items\n",
    "        \n",
    "        done = np.all(self.preselected == 1) # termination upon having inspected all items\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "train = True # CHANGE IF YOU WANT TO RE-TRAIN MODEL\n",
    "model_save_path = \"trained_knapsack_agent.pth\"\n",
    "\n",
    "# Training method variables\n",
    "if train:\n",
    "    episodes = 20\n",
    "    batch_size = 64\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10\n",
    "\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "    # Save trained agent\n",
    "    future_agent = NeuralNetworkAgent().to(device)\n",
    "    future_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    batch_index = 0\n",
    "    history_state = []\n",
    "    history_next_state = []\n",
    "    history_action = []\n",
    "    history_reward = []\n",
    "    history_done = []\n",
    "\n",
    "    # training episodes\n",
    "    for episode in range(episodes):\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "        N_items = 30 # Amount of items included in training\n",
    "        \n",
    "        # Initalise termination status and zero reward\n",
    "        ep_finished = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Get a new state\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        while not ep_finished: \n",
    "            # Epsilon greedy policy\n",
    "            if random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.forward(state)\n",
    "                    action = q_values.argmax().item()\n",
    "            else:\n",
    "                available_items = np.where(state_matrix[:,4] == 0)[0] #only consider selectable items\n",
    "                action = random.choice(available_items)\n",
    "            \n",
    "            next_state, reward, ep_finished = env.step(action)\n",
    "            next_state_matrix = next_state\n",
    "            next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "            \n",
    "            # Add step in episode to history\n",
    "            history_state.append(state)\n",
    "            history_next_state.append(next_state)\n",
    "            history_action.append(action)\n",
    "            history_reward.append(reward)\n",
    "            history_done.append(float(ep_finished))\n",
    "            \n",
    "            # Update to inspect next step\n",
    "            batch_index += 1\n",
    "            state = next_state\n",
    "            state_matrix = next_state_matrix\n",
    "\n",
    "            if batch_index >= batch_size:\n",
    "                q_valuestates = []\n",
    "                q_values_next_states = []\n",
    "\n",
    "                # Manually calculate q_values and next_q_values using your approach\n",
    "                for q_index in range(batch_index - batch_size, batch_index):\n",
    "                    q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                    next_q_value = future_agent(history_next_state[q_index]).max() # find optimal q-value\n",
    "                    q_valuestates.append(q_value)\n",
    "                    q_values_next_states.append(next_q_value)\n",
    "\n",
    "                q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "                q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "                rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "                # Calculate the target using next_q_values\n",
    "                with torch.no_grad():\n",
    "                    targets = rewards + (1.0 - dones) * gamma * q_values_next_states # Bellman equation\n",
    "\n",
    "                targets = targets.float()\n",
    "                loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "                # Update neural network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Store agent locally so it won't need to be trained again\n",
    "    torch.save(agent.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "else:\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    agent.load_state_dict(torch.load(model_save_path))\n",
    "    agent.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Manually calculate q_values and next_q_values using your approach\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_index \u001b[38;5;241m-\u001b[39m batch_size, batch_index):\n\u001b[1;32m---> 63\u001b[0m     q_value \u001b[38;5;241m=\u001b[39m agent(\u001b[43mhistory_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_index\u001b[49m\u001b[43m]\u001b[49m)[history_action[q_index]]\n\u001b[0;32m     64\u001b[0m     next_q_value \u001b[38;5;241m=\u001b[39m future_agent(history_next_state[q_index])\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;66;03m# find optimal q-value\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     q_valuestates\u001b[38;5;241m.\u001b[39mappend(q_value)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-3\n",
    "\n",
    "# Training method variables\n",
    "if train:\n",
    "    episodes = 100\n",
    "    batch_size = 32\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10\n",
    "\n",
    "    history_state = []\n",
    "    history_next_state = []\n",
    "    history_action = []\n",
    "    history_reward = []\n",
    "    history_done = []\n",
    "\n",
    "    # training episodes\n",
    "    for episode in range(episodes):\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "        N_items = 20 # Amount of items included in training\n",
    "        \n",
    "        # Initalise termination status and zero reward\n",
    "        ep_finished = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Get a new state\n",
    "        state_matrix = env.reset(N_items)\n",
    "        state = torch.FloatTensor(state_matrix).to(device)\n",
    "\n",
    "        while not ep_finished: \n",
    "            # Epsilon greedy policy\n",
    "            if random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.forward(state)\n",
    "                    action = q_values.argmax().item()\n",
    "            else:\n",
    "                available_items = np.where(state_matrix[:,4] == 0)[0] #only consider selectable items\n",
    "                action = random.choice(available_items)\n",
    "            \n",
    "            next_state, reward, ep_finished = env.step(action)\n",
    "            next_state_matrix = next_state\n",
    "            next_state = torch.FloatTensor(next_state_matrix).to(device)\n",
    "            \n",
    "            # Add step in episode to history\n",
    "            history_state.append(state)\n",
    "            history_next_state.append(next_state)\n",
    "            history_action.append(action)\n",
    "            history_reward.append(reward)\n",
    "            history_done.append(float(ep_finished))\n",
    "            \n",
    "            # Update to inspect next step\n",
    "            batch_index += 1\n",
    "            state = next_state\n",
    "            state_matrix = next_state_matrix\n",
    "\n",
    "            if batch_index >= batch_size:\n",
    "                q_valuestates = []\n",
    "                q_values_next_states = []\n",
    "\n",
    "                # Manually calculate q_values and next_q_values using your approach\n",
    "                for q_index in range(batch_index - batch_size, batch_index):\n",
    "                    q_value = agent(history_state[q_index])[history_action[q_index]]\n",
    "                    next_q_value = future_agent(history_next_state[q_index]).max() # find optimal q-value\n",
    "                    q_valuestates.append(q_value)\n",
    "                    q_values_next_states.append(next_q_value)\n",
    "\n",
    "                q_valuestates = torch.stack(q_valuestates).to(device)\n",
    "                q_values_next_states = torch.stack(q_values_next_states).to(device)\n",
    "\n",
    "                rewards = torch.tensor(history_reward[-batch_size:], dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(history_done[-batch_size:], dtype=torch.float32).to(device)\n",
    "\n",
    "                # Calculate the target using next_q_values\n",
    "                with torch.no_grad():\n",
    "                    targets = rewards + (1.0 - dones) * gamma * q_values_next_states # Bellman equation\n",
    "\n",
    "                targets = targets.float()\n",
    "                loss = nn.MSELoss()(q_valuestates, targets)\n",
    "\n",
    "                # Update neural network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Episode {episode} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Store agent locally so it won't need to be trained again\n",
    "    torch.save(agent.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "else:\n",
    "    env = Knapsack_environment()\n",
    "    agent = NeuralNetworkAgent().to(device)\n",
    "    agent.load_state_dict(torch.load(model_save_path))\n",
    "    agent.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
